  <html>

  <head>
    <style>
      body {
        margin: 0.5in;
        background-color: aliceblue
      }

      h1,
      h2,
      h3,
      h4,
      h5 {
        font-family: helvetica, arial, sans-serif
      }

      .cal {
        border-collapse: collapse;
      }

      .cal td {
        border-bottom: 1px solid black;
        vertical-align: top;
        padding: 3px 5px;
      }
    </style>
  </head>

  <h1>DS 4440 - Spring 2024 - Practical Deep Networks</h1>

  <p>Tuesdays 11:45 AM - 1:25 PM and Thursdays 2:50 PM - 4:30 PM, <a
      href="https://nuflex.northeastern.edu/classroom/sh-420/">Shillman Hall 420</a>. Class meetings are in-person, and
    will sometimes be on zoom at <a href="https://northeastern.zoom.us/my/davidbau">this link</a>.

  <p>Professor: David Bau <a href="mailto:davidbau@northeastern.edu">davidbau@northeastern.edu</a>.
    Office Hours: before class, 10:30am Tuesdays, near the lecture hall

  <p>TAs:<br>
    Eric Todd
    <a href="malto:todd.er@northeastern.edu">todd.er@northeastern.edu</a>. Help hours: Wednesdays 10am-1pm, Fridays 3:30-4:30pm (<a
      href="https://northeastern.zoom.us/my/erictodd">zoom link</a>)<br>
    Arnab Sen Sharma
    <a href="mailto:sensharma.a@northeastern.edu">sensharma.a@northeastern.edu</a>. Help hours: Mondays 4-6pm (<a
      href="https://northeastern.zoom.us/my/arnab.sensharma">zoom link</a>)


  <p>Sign up for Piazza
    <a href="https://piazza.com/northeastern/spring2024/ds444039306202430">here</a>.
    Hand in reading questions (the day before class) and class notebooks (after class) on Canvas
  <a href="https://northeastern.instructure.com/courses/166739/assignments">here</a>.
  Homeworks will go to
  <a href="https://www.gradescope.com/courses/700584">gradescope here</a>.

<h2>Summary</h2>
In this course we will learn the principles and practice of deep learning methods.

We will cover the capabilities of deep networks, the main methods for training them effectively, and the common
architectures and techniques for using a deep network to process and produce images and natural language text. In
addition to getting experience using these methods, we will discuss the evolution of the field, some of the main
questions and debates that have emerged in deep learning research, and explore some current research topics. There will
be a final project where you work with a partner implement a solution to a problem using deep network methods.

<h2>Grading</h2>
160 points total.

<p>Class participation: 40 points. In-class participation and notebooks.
  There will be points for submitting questions on nightly (before-class) reading, and for submitting in-class
  notebooks, about 20 each.

<p>Homework: 40 points. Programming and calculation exercises.

  Homeworks will be jupyter notebooks to be worked through by students individually, due to be submitted online every
  two weeks at 11AM before class when they are due. Late homework submissions will be accepted but will lose points per
  day late, no points after a week. 10 points per homework.

<p>Midterm: 40 points. A written exam about foundational methods. Closed-book.

<p>Final Project: 40 points. A project to solve a problem by training and evaluating a deep network. Done by groups of 2
  students (3 with permission). We may choose a contest activity for this.

<h2>Books</h2>
<p>There is no textbook for the class. Slides of material we have covered are linked below.
  There are original-research paper readings, linked below.

<p>
  For those looking for a good reference book covering similar material, here are some good ones I recommend. Both arre
  available online.
<ul>
  <li><a href="https://fleuret.org/francois/lbdl.html">Fleuret's Little Book of Deep Learning</a>
  <li><a href="https://d2l.ai/">Dive into Deep Learning</a>
</ul>

<h2>Calendar</h2>
<table class="cal">
  <tr>
    <td>Tuesday, January 9, 2024</td>
    <td>What is deep learning, a bit of history in three traditions</td>
    <td>
      <a href="slides/Lecture-1-Introduction.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/also/NyTimes-1958.pdf">1958 In-class Reading</a> -
      <a href="https://colab.research.google.com/github/davidbau/ds4440/blob/main/classdemo/PerceptronLearning.ipynb">Perceptrons
        Notebook</a>
      (<a href="https://northeastern.instructure.com/courses/166739/quizzes/581798">hand-in</a>) -
      <a href="https://colab.research.google.com/github/davidbau/ds4440/blob/main/HW1/HW1_Introduction.ipynb">HW1
        out</a>
    </td>
  </tr>
  <tr>
    <td>Thursday, January 11, 2024</td>
    <td>Continuing history: gradient descent, universal computation, what is an MLP</td>
    <td>
      <a href="slides/Lecture-2-Rumelhart-pytorch.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/Rumelhart-1986.pdf">Read Rumelhart 1986</a>
      (<a href="https://northeastern.instructure.com/courses/166739/quizzes/581707">hand-in</a>) -
      <a
        href="https://colab.research.google.com/github/davidbau/how-to-read-pytorch/blob/master/notebooks/1-Pytorch-Introduction.ipynb">Notebook
        on How to Read Pytorch</a> (<a
        href="https://northeastern.instructure.com/courses/166739/assignments/2180620">hand-in</a>)
    </td>
  </tr>
  <tr>
    <td>Tuesday, January 16, 2024</td>
    <td>How to program a multilayer perceptrons: pytorch and modular machine learning</td>
    <td>
      <a href="slides/Lecture-3-MLP-Modular-Learning.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/Bottou-1990.pdf">Read Bottou 1990</a>
      (<a href="https://northeastern.instructure.com/courses/166739/quizzes/582037">hand-in</a>) -
      <a href="https://bit.ly/howto-pytorch-3">Notebooks: Pytorch-3-Autograd</a>
      <a href="https://bit.ly/howto-pytorch-4">Pytorch-4-Modules</a>
      (<a href="https://northeastern.instructure.com/courses/166739/assignments/2180620">no new hand-in</a>) -
      <a href="https://colab.research.google.com/github/davidbau/ds4440/blob/main/HW2/HW2_Backpropagation.ipynb">HW2
        out</a>
    </td>
  </tr>
  <tr>
    <td>Thursday, January 18, 2024</td>
    <td>Backprop I</td>
    <td>
      <a href="slides/Lecture-4-Losses-Backprop.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/Solla-1988.pdf">Read Solla 1988</a> -
      <a href="https://bit.ly/ce-vs-mse">Notebook: cross-entropy</a>
      <a href="https://bit.ly/comp-graph">Notebook: computation graphs</a>
      (<a href="https://northeastern.instructure.com/courses/166739/assignments/2185893">hand-in</a>) -
      <a href="https://northeastern.instructure.com/courses/166739/assignments/2174624">HW1 due</a> -
      <a href="https://colab.research.google.com/github/davidbau/ds4440/blob/main/HW3_Training_NN/HW3_Training_NN.ipynb">HW3 out</a>
    </td>
  </tr>
  <tr>
    <td>Tuesday, January 23, 2024</td>
    <td>Backprop II, SGD, and Momentum</td>
    <td>
      <a href="slides/Lecture-5-Backprop-ADAM.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/Kingma-2015.pdf"
        >Read Kingma 2015 (ADAM)</a> -
      <a href="https://bit.ly/backprop-colab">Notebook: backprop</a>
      (<a href="https://northeastern.instructure.com/courses/166739/assignments/2188330"
        >hand-in</a>)
    </td>
  </tr>
  <tr>
    <td>Thursday, January 25, 2024</td>
    <td>Practical training techniques: ways to improve gradients and ways to regularize</td>
    <td>
      <a href="slides/Lecture-6-ADAM-Init.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/Glorot-2010.pdf"
        >Read Glorot 2010 (Xavier init)</a> -
      <a href="https://bit.ly/cs7150-zigzag">Notebook: zigzag</a>
      <a href="https://bit.ly/stuck-optim">Notebook: stuck optimization</a> (<a href="https://northeastern.instructure.com/courses/166739/assignments/2190661">hand-in</a>) -
      <a href="https://northeastern.instructure.com/courses/166739/assignments/2183942">HW2 due</a>
    </td>
  </tr>
  <tr>
    <td>Tuesday, January 30, 2024</td>
    <td>
      Bau Lab Research - Large Model Interpretability
    </td>
    <td>
      <a href="slides/Lecture-7-Interp-Research.pptx">slides</a> - [no notebook or reading hand-in]
    </td>
  </tr>
  <tr>
    <td>Thursday, February 1, 2024</td>
    <td>The practice of training (2). Initialization.  Then: Generalization and regularization</td>
    <td>
      <a href="slides/Lecture-8-Training-Techniques.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/Krogh-1991.pdf"
        >Read Krogh 1991 (Weight decay)</a> (<a href="https://northeastern.instructure.com/courses/166739/quizzes/584077">hand-in</a>) -
      <a href="https://bit.ly/training-dyn">Notebook: training dynamics</a> (<a href="https://northeastern.instructure.com/courses/166739/assignments/2191812">hand-in</a>)
    </td>
  </tr>
  <tr>
    <td>Tuesday, February 6, 2024</td>
    <td>Words and images: representing concepts as vectors; time-invariant and translation-invariant weight-sharing</td>
    <td>
      <a href="slides/Lecture-9-Convolutions.pptx">slides</a> -
      <a href="https://papers.baulab.info/papers/LeCun-1989.pdf">Read LeCun 1989 (CNNs)</a> (<a href="https://northeastern.instructure.com/courses/166739/quizzes/584257">hand-in</a>) -
      <a href="https://bit.ly/vis-conv">Notebook: convolutions</a> (<a href="https://northeastern.instructure.com/courses/166739/assignments/2193907">hand-in</a>) -
      <a href="https://northeastern.instructure.com/courses/166739/assignments/2183956">HW3 due</a>
    </td>
  </tr>
  <tr>
    <td>Thursday, February 8, 2024</td>
    <td>From invariance to convolution: CNNs; receptive fields</td>
    <td>
      <a href="https://papers.baulab.info/papers/Krizhevsky-2012.pdf">Read AlexNet 2012</a> (<a href="https://northeastern.instructure.com/courses/166739/quizzes/584678">hand-in</a>) -
    </td>
  </tr>
  <tr>
    <td>Tuesday, February 13, 2024</td>
    <td>SOTA CNNs; resnets, unets</td>
  </tr>
  <tr>
    <td>Thursday, February 15, 2024</td>
    <td>From invariance to recurrence: RNNs</td>
  </tr>
  <tr>
    <td>Tuesday, February 20, 2024</td>
    <td>RNNs and language modeling (the statistcial factorization of language)</td>
  </tr>
  <tr>
    <td>Thursday, February 22, 2024</td>
    <td>Midterm review</td>
  </tr>
  <tr>
    <td>Tuesday, February 27, 2024</td>
    <td>Midterm exam</td>
  </tr>
  <tr>
    <td>Thursday, February 29, 2024</td>
    <td>Transformers</td>
  </tr>
  <tr>
    <td>Tuesday, March 5, 2024</td>
    <td>Spring break</td>
  </tr>
  <tr>
    <td>Thursday, March 7, 2024</td>
    <td>Spring break</td>
  </tr>
  <tr>
    <td>Tuesday, March 12, 2024</td>
    <td>After transformers: S4 and Mamba</td>
  </tr>
  <tr>
    <td>Thursday, March 14, 2024</td>
    <td>Generating images: VAE and GANs</td>
  </tr>
  <tr>
    <td>Tuesday, March 19, 2024</td>
    <td>Generating images: Diffusion and text-condition diffusion. Stable diffusion</td>
  </tr>
  <tr>
    <td>Thursday, March 21, 2024</td>
    <td>LLMs: how to make a chatbot. pretraining and scaling laws; fine-tuning and human prefs, RLHF.</td>
  </tr>
  <tr>
    <td>Tuesday, March 26, 2024</td>
    <td>Machine learning without machine learning: zero-shot QA, ICL, prompting, chain-of-thought.</td>
  </tr>
  <tr>
    <td>Thursday, April 4, 2024</td>
    <td>Interpreting large models: ROME lecture</td>
  </tr>
  <tr>
    <td>Tuesday, April 9, 2024</td>
    <td>Agents, tool use, and emergence of reasonable behavior</td>
  </tr>
  <tr>
    <td>Thursday, April 11, 2024</td>
    <td>Extra lecture or gap day</td>
  </tr>
  <tr>
    <td>Tuesday, April 16, 2024</td>
    <td>Project presentations</td>
  </tr>

</table>
