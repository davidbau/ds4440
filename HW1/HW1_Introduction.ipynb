{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1. Foundations of Neural Networks\n",
        "\n",
        "## Learning Objective\n",
        "\n",
        "The goal of this homework is to get you familar with some of the foundational mathematical and programming tools used in deep learning, so we will review a little bit of calculus and linear algebra."
      ],
      "metadata": {
        "id": "qRhCn6A1rbMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: McCulloch-Pitts Neural Networks\n",
        "\n",
        "The modern conception of artificial neural networks is essentially the same as the model originally devised by [McCulloch and Pitts in their seminal 1943 paper](https://papers.baulab.info/papers/McCullochPitts-1943.pdf).  In that work, they observed that a biological neuron could be seen as an object that adds up its inputs, possibly weighting some inputs differently from others, and then firing an output only once some threshold is reached.  This can be modeled as a weighted sum followed by a nonlinearity:\n",
        "    \n",
        "<img src=\"https://cs7150.baulab.info/2022-Fall/hw1/mp-model.png\" width=800>\n",
        "    \n",
        "McCullough and Pitts reasoned about such neurons individually or in very small networks, and they asked: what is the computational power of such networks? Can they reproduce any logical computation? In this exercise we will follow along with their exploration by constructing networks for the various logical operations that can be created with two binary inputs."
      ],
      "metadata": {
        "id": "RwNZUE3CsR8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall examples of these logical operators include: AND ($\\land$), OR ($\\lor$), NOT ($\\neg$), and XOR ($\\oplus$). We've provided tables below as a refresher for how these functions behave.\n",
        "\n",
        "<table>\n",
        "<tr><th>A AND B ($A \\land B$)</th><th>A OR B ($A \\lor B$)</th><th>NOT A ($\\neg A$)</th><th>A XOR B ($A \\oplus B$)</th></tr>\n",
        "<tr><td>\n",
        "\n",
        "Input 1 (A) | Input 2 (B) | Output\n",
        "--------|---------|---------\n",
        "0  | 0 | 0\n",
        "1  | 0 | 0\n",
        "0  | 1 | 0\n",
        "1  | 1 | 1\n",
        "\n",
        "</td><td>\n",
        "\n",
        "Input 1 (A)  | Input 2 (B) | Output\n",
        "-------|------|------\n",
        "0  | 0 | 0\n",
        "1  | 0 | 1\n",
        "0  | 1 | 1\n",
        "1  | 1 | 1\n",
        "\n",
        "</td><td>\n",
        "\n",
        "Input 1 (A)  | Input 2 (B) | Output\n",
        "-------|------|------\n",
        "0  | 0 | 1\n",
        "1  | 0 | 0\n",
        "0  | 1 | 1\n",
        "1  | 1 | 0\n",
        "\n",
        "</td>\n",
        "<td>\n",
        "\n",
        "Input 1  (A)| Input 2 (B)| Output\n",
        "-------|------|------\n",
        "0  | 0 | 0\n",
        "1  | 0 | 1\n",
        "0  | 1 | 1\n",
        "1  | 1 | 0\n",
        "\n",
        "</td>\n",
        "</tr> </table>\n",
        "\n",
        "\n",
        "As a starting point to examine the expressive power of the McCulloch-Pitts Neuron, we will see whether it can model simple logical functions.\n",
        "\n",
        "Review the basic implementation of a neuron provided in the code below. It takes two binary inputs and produces a binary output.\n",
        "To compute the output, the neuron (1) multiplies its weights, $w_1$ and $w_2$, with their corresponding inputs $x_1$ and $x_2$, (2) adds the bias term, and (3) applies a simple threshold nonlinearity.\n"
      ],
      "metadata": {
        "id": "Isy0vDo9vUJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryNeuron:\n",
        "    def __init__(self, weight_1, weight_2, bias):\n",
        "        \"\"\"\n",
        "        Store the Neuron's weights, bias, and activation functions\n",
        "        \"\"\"\n",
        "        self.weight_1 = weight_1\n",
        "        self.weight_2 = weight_2\n",
        "        self.bias = bias\n",
        "\n",
        "    def __call__(self, input_1, input_2):\n",
        "        \"\"\"\n",
        "        Accepts two binary inputs and computes the output of the neuron using the initialized weights and bias\n",
        "        \"\"\"\n",
        "        assert input_1 in [0,1] or isinstance(input_1, bool), \"Input 1 is not a binary input\"\n",
        "        assert input_2 in [0,1] or isinstance(input_2, bool), \"Input 2 is not a binary input\"\n",
        "\n",
        "        weighted_sum = self.weight_1  * input_1  + self.weight_2 * input_2 + self.bias\n",
        "        return self.activation(weighted_sum)\n",
        "\n",
        "    def activation(self, x):\n",
        "        \"\"\"\n",
        "        A simple nonlinearity - also known as the heaviside step function.\n",
        "        The function takes x as input, and returns 1 if x is nonnegative, 0 otherwise.\n",
        "        \"\"\"\n",
        "        if x >= 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ],
      "metadata": {
        "id": "gfurwry9xArs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 1.1:**\n",
        "- By carefully choosing values for the weights $w_1$, $w_2$, and the bias term, show that the McCulloch-Pitts Neuron can accurately model each of the following logical operators: (1) A AND B, (2) A OR B, (3) NOT A, (4) A AND NOT B. For each of these cases, fill in weight and bias values into the table below such that the `BinaryNeuron` properly models the corresponding logical operator.\n",
        "- You may use the `BinaryNeuron` from above to check your work.\n",
        "- Hint: While there are many valid solutions, it may be helpful to restrict the values of your weights and biases to be within the range [-2,2]."
      ],
      "metadata": {
        "id": "UOhp4eNEEST7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and show your work (create new cells as needed):\n",
        "weight_1 = None\n",
        "weight_2 = None\n",
        "bias = None\n",
        "BN = BinaryNeuron(weight_1, weight_2, bias)\n",
        "\n",
        "print(f\"Input\\t Output\\n-------   -----\\nin:[0,0]  out:{BN(0,0)}\\nin:[1,0]  out:{BN(1,0)}\\nin:[0,1]  out:{BN(0,1)}\\nin:[1,1]  out:{BN(1,1)}\\n\")"
      ],
      "metadata": {
        "id": "TKAvQJo0Rd0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.1 Answer:**\n",
        "<table>\n",
        "<tr><th>A AND B ($A \\land B$)</th><th>A OR B ($A \\lor B$)</th><th>NOT A ($\\neg A$)</th><th>A AND NOT B ($A \\land (\\neg B)$)</th></tr>\n",
        "<tr><td>\n",
        "\n",
        "Weight 1 ($w_1$) | Weight 2 ($w_2$) | Bias\n",
        "--------|---------|---------\n",
        "FILL IN HERE| FILL IN HERE | FILL IN HERE\n",
        "\n",
        "\n",
        "</td><td>\n",
        "\n",
        "Weight 1 ($w_1$) | Weight 2 ($w_2$) | Bias\n",
        "--------|---------|---------\n",
        "FILL IN HERE| FILL IN HERE | FILL IN HERE\n",
        "\n",
        "</td><td>\n",
        "\n",
        "Weight 1 ($w_1$) | Weight 2 ($w_2$) | Bias\n",
        "--------|---------|---------\n",
        "FILL IN HERE| FILL IN HERE | FILL IN HERE\n",
        "\n",
        "</td><td>\n",
        "\n",
        "Weight 1 ($w_1$) | Weight 2 ($w_2$) | Bias\n",
        "--------|---------|---------\n",
        "FILL IN HERE| FILL IN HERE | FILL IN HERE\n",
        "\n",
        "</td>\n",
        "</tr> </table>"
      ],
      "metadata": {
        "id": "0rTzwdg7RUiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Most Two-Input Logical Operators are Linearly Separable\n",
        "\n",
        "One way to think about these logical operators is as simple classification problems where there are 4 input data points, (0,0), (1,0), (0,1), (1,1) that have different labels depending on the operator.\n",
        "\n",
        "Under this setting, a neuron can be thought of as a simple classifier of these data points. Our `BinaryNeuron` produces a binary response by thresholding its internal activation at 0. The neuron produces an output of 1 if $w_1x_1 + w_2x_2 + b \\geq 0$, and otherwise produces 0.\n",
        "\n",
        "With some manipulation, we can rewrite this inequality to show that the `BinaryNeuron` uses a line that acts a decision boundary between outputting a 1 or a 0. We say that a dataset is **linearly separable** if there exists a line that separates the datapoints that have different labels (i.e. all data points with the label 0 are on one side of the line, and all data points with label 1 are on the other side).\n",
        "\n",
        "\n",
        "**Question 1.2.1:**\n",
        "- Derive an equation for a line that separates the data points for the AND logical operator. It may be helpful to write it in slope-intercept form.\n",
        "- Using the weight and bias values you provided above, plot the line that defines the decision boundary of your neuron for the AND logical operator, as well as the data points colored according to their output label.\n",
        "\n",
        "**Question 1.2.2:**\n",
        "- It turns out that the McCulloch-Pitts Neuron can perfectly model almost any logical operator with two inputs, except for XOR (and XNOR). Provide an explanation in the space provided below as to why XOR can't be modeled by a single neuron.\n"
      ],
      "metadata": {
        "id": "Orpt-ywGqkNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.2.1 Answer:**\n",
        "\n",
        "- Edit Response Here, and the Code Snippet Below"
      ],
      "metadata": {
        "id": "4uKgTcvwRhm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For the AND logical operator:\n",
        "# 1. Plot the separating line\n",
        "# 2. Plot the data points, colored by class label\n",
        "# 3. Don't forget to add a legend, title, axis labels, etc."
      ],
      "metadata": {
        "id": "9_M90vtD0Zuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.2.2 Answer:**\n",
        "\n",
        "- Edit Response Here"
      ],
      "metadata": {
        "id": "raCDXQQh0ZLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Algebra Review\n",
        "Given two vectors $w = [w_1,w_2, \\ldots, w_n]^{T}$ and $x = [x_1, x_2, \\ldots, x_n]^{T} \\in \\mathbb{R}^{n}$, recall that the **dot product** between $w$ and $x$ can be written as the following sum: $w^{T}x = \\sum_{1}^{n}{w_i \\cdot x_i}$.\n",
        "Thus, we can think of any weighted sum as a dot product between a weight vector $w$ and an input vector $x$, where each $w_i$ is the corresponding weight placed on the $i$th element of $x$.\n",
        "\n",
        "Recall that the **norm** of a vector $v \\in R^{n}$ is written as $||v|| = \\sqrt{v_1^{2} + \\ldots + v_n^{2}}$. We say a vector has **unit length**, or is a **unit vector** when its norm is equal to 1, (i.e. $||v|| = 1$). We can normalize any vector to have unit length by dividing by its norm, $u = \\frac{v}{||v||}$.\n",
        "\n",
        "\n",
        "For any two unit vectors, we can measure how \"similar\" they are by the angle between them. One way to do this is via **cosine similarity** which is defined between two vectors $x$ and $y$, and is written as: $cos(\\theta) = \\frac{x ^{T} y}{||x||\\cdot||y||}$, where $\\theta$ represents the angle between $x$ and $y$.\n",
        "\n",
        "\n",
        "\n",
        "**Question 1.3:**\n",
        "Recall the definition of the McCulloch-Pitts neural network as shown in the figure above.\n",
        "- How might you write the output using a dot product?\n",
        "- How many vectors are involved, and what dimension are they?\n",
        "\n",
        "**Question 1.4:**\n",
        "Suppose you are given a neuron that takes n inputs with weights $w_i$ and bias $b$, where the input vector $x \\in \\mathbb{R}^{n}$, has norm $||x|| = 1$ and components $x_i$. Suppose also that this neuron has the following activation function: $\\sigma(x) = x$, instead of a nonlinearity.\n",
        "- Which choice of x would maximize the output of the neuron?\n",
        "- What is the cosine of the angle between this vector $x$ and the vector $w$?"
      ],
      "metadata": {
        "id": "BedUsEvFxGVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.3 Answer:**\n",
        "\n",
        "-  Edit Response Here\n",
        "\n",
        "\n",
        "**Question 1.4 Answer:**\n",
        "\n",
        "-  Edit Response Here"
      ],
      "metadata": {
        "id": "tdBMHIIY3JsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Neurons as Classifiers\n",
        "\n",
        "In this exercise, we will explore using neurons to distinguish between two classes of images - (1) cats and (2) jellyfish.\n",
        "\n",
        "\n",
        "\n",
        "Decades after McCulloch and Pitts, researchers like [Sarah Solla](https://papers.baulab.info/papers/Solla-1988.pdf) and [John Hopfield](https://papers.baulab.info/papers/also/Hopfield-1987.pdf) discovered that networks are very effective when trained to model *probabilities* instead of just discrete binary logic.  Even in the case where the output should make a choice between two alternatives, it is often best to have the network output its estimate of the *probability distribution* of the choice to be made, rather than just a 0 or a 1.\n",
        "\n",
        "So in modern deep learning, we will often pursue the goal of matching some true vector of discrete probabilities $y \\in \\mathbb{R}^{n}$ (where $n$ is the number of classes) by computing some model-predicted vector of probabilities $p \\in \\mathbb{R}^{n}$ that is derived from some raw neural network output $z \\in \\mathbb{R}^{n}$, and then measuring its deviation from the true distribution $y$.\n",
        "\n",
        "For this exercise we will use the sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ to estimate a probability because it squashes input values between 0 and 1.\n",
        "\n",
        "<!-- Note: Sigmoid is no longer a commonly-used activation function, largely due to its effect on propogating gradients, but we will discuss other approaches to converting activations into probability distributions (softmax and cross-entropy) later in the course. -->\n",
        "\n",
        "<!-- ...\n",
        "\n",
        "Experience w/ pytorch, etc. -->"
      ],
      "metadata": {
        "id": "TJDU1H7Lw2_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the Image Dataset\n",
        "\n",
        "Run the following code to download the image dataset and display a sample image for each class.\n",
        "\n",
        "A black and white image is typically represented as a 2d array that has height x width pixels. While the highest resolution of images provided are 256x256 pixels, we will be working with lower resolution images (6x6) to reduce the number of inputs to our neural network."
      ],
      "metadata": {
        "id": "ljjsc1RH5GTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX6ku4bOH2be"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U git+https://github.com/davidbau/baukit@main#egg=baukit\n",
        "!rm -f catfish.zip\n",
        "!wget https://ds4440.baulab.info/data/catfish.zip\n",
        "!unzip -o catfish.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, Grayscale, ToTensor, Normalize\n",
        "\n",
        "AsSignedTensor = Compose([Grayscale(num_output_channels=1), ToTensor(), Normalize(0.5, 0.5)])\n",
        "ds = ImageFolder('catfish/size_6/train', transform=AsSignedTensor)\n",
        "dsv = ImageFolder('catfish/size_6/val', transform=AsSignedTensor)\n",
        "ds2 = ImageFolder('catfish/size_256/train', transform=AsSignedTensor)"
      ],
      "metadata": {
        "id": "HvQc_E6AIQk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from baukit.renormalize import as_image\n",
        "from baukit import show\n",
        "pixelated = show.style(height=256, width=256, imageRendering='pixelated')\n",
        "\n",
        "for imgnum in [2, 102]:\n",
        "    im, c = ds[imgnum]\n",
        "    im2, c2 = ds2[imgnum]\n",
        "    show([[pixelated, as_image(im), as_image(im2)]])"
      ],
      "metadata": {
        "id": "HLZfdKzbJco7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Flatten Code\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def show_flatten():\n",
        "    plt.subplot(121)\n",
        "    # Create a tensor with numbers 1-16\n",
        "    numbers = torch.arange(1, 17).view(4, 4)\n",
        "\n",
        "    canvas = torch.ones(16).view(4,4)\n",
        "\n",
        "    # Plot the grid\n",
        "    plt.imshow(canvas, cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            plt.text(j, i, str(numbers[i, j].item()), ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "    border = patches.Rectangle((-0.5, -0.5), 4, 4, linewidth=2, edgecolor='black', facecolor='none')\n",
        "    plt.gca().add_patch(border)\n",
        "\n",
        "    # Add black gridlines\n",
        "    for i in range(5):\n",
        "        plt.axhline(i - 0.5, color='black', linewidth=1)\n",
        "        plt.axvline(i - 0.5, color='black', linewidth=1)\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.title('4x4 Image w/ 16 Pixels')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(122)\n",
        "\n",
        "    # Create a tensor with numbers 1-16\n",
        "    numbers = torch.ones(16).view(16, 1)\n",
        "\n",
        "    # Plot the row with white background and black text\n",
        "    plt.imshow(numbers, cmap='gray', vmin=0, vmax=1, aspect=1.3)\n",
        "\n",
        "    # Add text annotations with black color\n",
        "    for j in range(16):\n",
        "        plt.text(0, j, str(j+1), ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "    # Add black gridlines\n",
        "    for i in range(2):\n",
        "        plt.axvline(i - 0.5, color='black', linewidth=1)\n",
        "    for i in range(17):\n",
        "        plt.axhline(i - 0.5, color='black', linewidth=1)\n",
        "\n",
        "    plt.title('16x1 Flattened Image Vector w/ 16 Pixels')\n",
        "    plt.axis('off')\n",
        "    plt.grid('all')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "U673pq2wBgg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flattening a 2d Image Array into a vector\n",
        "show_flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "VYcqp2JJ8Zsq",
        "outputId": "f6dd4eb6-cb5a-4871-98c4-7ae6b51584c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discovering Neuron Weights when Inputs are Larger\n",
        "\n",
        "\n",
        "We want to create a neural network that is able to predict whether a given image contains a cat or a jellyfish.\n",
        "\n",
        "Let's denote the data for the $i$th image in the data set as the vector $x_{i}$. Since image is a flattened vector of pixel values, $x_{i}$ is a vector with a number for each pixel.  We'll write $x_{ij}$ for the value of the $j$th pixel in the $i$th image.\n",
        "\n",
        "Our neural network estimates that the \"$x_i$ is-a-Cat\" probability $p_i$ when given input $x_i$ by computing the following two things.  First, using its weights $w$ it does a linear operation (a weighted sum over the pixels of $x_i$) to get a score:\n",
        "\n",
        "$$s_i = w \\cdot x_i = \\sum_j w_j x_{ij}$$\n",
        "\n",
        "And then it squishes the score to the range [0, 1] using a nonlinearity (the sigmoid function), so we can interpret the output of the network as a probability.\n",
        "\n",
        "$$p_i = \\sigma(s_i) = \\frac{1}{1 + e^{-s_i}}$$\n",
        "\n",
        "In our case, we can flatten an image into a single vector so that the neuron takes all of the pixels as input. See example above of how we flatten an image into a vector. The number of pixels in an image denoted above as $j$.\n",
        "\n",
        "Note: This formulation removes the bias term, but we can easily add it back in, or concatenate it as part of our weight vector as $w_{n+1}$ with an additional implicit input of 1 as $x_{n+1}$."
      ],
      "metadata": {
        "id": "3bwtIZsQUYyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.1**\n",
        "\n",
        "Implement functions for a Neuron:\n",
        "\n",
        "- `init_neuron_weights` function: It should initialize a neuron weight vector and a bias term each as a torch tensor of zeros, and return them\n",
        "- `score` function: Computes the dot product of the weights with the input and add the bias term.\n",
        "- `run_neuron` function: This computes the output of the neuron, by applying the sigmoid activation function to the score.\n",
        "\n",
        "Note: You can flatten the input image with `.flatten()` to turn it into a vector with a single dimension."
      ],
      "metadata": {
        "id": "vCXzwPghLtR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(s_i):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function\n",
        "    \"\"\"\n",
        "    return 1 / (1 + torch.exp(-s_i))\n",
        "\n",
        "def init_neuron_weights(input_size):\n",
        "    \"\"\"\n",
        "    Initialize pytorch tensors representing the neuron weights and bias\n",
        "    \"\"\"\n",
        "    neuron_weights = None # Replace with torch tensor of appropriate size and initialization\n",
        "    neuron_bias = None # Replace with torch tensor of appropriate size and initialization\n",
        "    return neuron_weights, neuron_bias\n",
        "\n",
        "def score(neuron_weights, neuron_bias, x_i):\n",
        "    \"\"\"\n",
        "    Computes the dot product of the weights with the input and adds the bias term.\n",
        "    \"\"\"\n",
        "    s_i = None # Replace with appropriate code. Don't forget to flatten the input image!\n",
        "    return None\n",
        "\n",
        "def run_neuron(neuron_weights, neuron_bias, x_i):\n",
        "    \"\"\"\n",
        "    Applies a sigmoid to the score of an input vector.\n",
        "    \"\"\"\n",
        "    p_i = None # Replace with appropriate code to compute the probability of instance x_i.\n",
        "    return None"
      ],
      "metadata": {
        "id": "h0BLT20_LtkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximum Likelihood Estimation\n",
        "\n",
        "While we initialize the weights of our neuron to all have a value of zero, in practice we want to find the values of neuron weights that maximize its estimate of the total log probability of the entire data set. This form of parameter estimation is called **Maximum Likelihood Estimation (MLE)**.\n",
        "\n",
        "To do this we follow a usual convention in neural networks where we define a \"Loss\" which we want to minimize. Because we want to maximize the log probability, we we can define our loss $L$ as the negative log probability, and we'll minimize that since minimizing a negative is the same as maximizing a positive.\n",
        "\n",
        "Let $y_i$ be 1 if the instance $x_i$ is a Cat and $y_i=0$ if $x_i$ is a Jellyfish. Then, for an individual instance if the probability of it being a Cat is p_i, the probability of it being a Jellyfish (denoted Jelly below) is 1-p_i. We can write this as $p_i^{y_i} * (1-p_i)^{1-y_i}$. We assume all instances are independent, which means we can write the total probability as a product of the individual probabilities: $\\prod_i P_{pred}(x_i) = \\prod_i p_i^{y_i}  (1-p_i)^{1-y_i}$.\n",
        "\n",
        "To compute the log likelihood, we take the log of this product, we can turn the product into a sum as we pull the log inside to give us $\\sum_i{log(p_i^{y_i}(1-p_i)^{(1-y_{i})})}$. And simplifying some more, we know this is equivalent to $\\sum_i{y_ilog(p_i) + (1-y_i)log(1-p_{i})}$. To get the negative log likelihood, we multiply by negative 1, and then we can filter the classes based on when $y_i=1$, and $y_i=0$, since many of the terms in the sum become zero otherwise. This gives us that the negative log likelihood $L$ can be written as\n",
        "\n",
        "$$L = - \\log \\prod_i P_{pred}(x_i) = -\\sum_{i \\in \\text{Cat}} \\log p_i - \\sum_{i \\in \\text{Jelly}} \\log (1- p_i)$$\n",
        "\n",
        "**Question 2.2**\n",
        "- Write a function that computes the log probability of a single data point\n",
        "- Write a function that computes the negative log likelihood of the entire dataset"
      ],
      "metadata": {
        "id": "1Usq54R7LliV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logprob_of_data_point(input_image, class_label, neuron_weights, neuron_bias):\n",
        "    \"\"\"\n",
        "    Compute the Log Probability of an individual instance.\n",
        "    input_image is an x_i, and class_label is a y_i.\n",
        "    \"\"\"\n",
        "    p_i = None # Replace with appropriate code\n",
        "    if class_label == 1:\n",
        "        return None # Replace with appropriate code\n",
        "    else:\n",
        "        return None # Replace with appropriate code\n",
        "\n",
        "def negative_logprob_of_dataset(dataset, neuron_weights, neuron_bias):\n",
        "    \"\"\"\n",
        "    Compute the total negative log probability of the provided dataset. This is the loss function L.\n",
        "    \"\"\"\n",
        "    total_negative_logprob = None # Replace with appropriate code\n",
        "    return total_negative_logprob"
      ],
      "metadata": {
        "id": "G4kK8qkWQ3mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code - You should get 50% accuracy before any optimization of the weights\n",
        "\n",
        "neuron_weights, neuron_bias = init_neuron_weights()\n",
        "\n",
        "def accuracy_of_data_set(dataset, neuron_weights, neuron_bias):\n",
        "    return sum((run_neuron(neuron_weights,neuron_bias,x_i) > 0.5) == y_i for x_i,y_i in dataset) / len(dataset)\n",
        "\n",
        "print(f'Initial Neg log likelihood {negative_logprob_of_dataset(ds, neuron_weights, neuron_bias)}')\n",
        "print(f'Initial Accuracy {accuracy_of_data_set(ds, neuron_weights, neuron_bias)}')"
      ],
      "metadata": {
        "id": "PyMbJiZ1YbWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimizing Negative Log Likelihood\n",
        "\n",
        "Recall from calculus that to minimize a function we can look at where its derivative is zero. We will calculate the derivative with respect to the neuron weights, and then we will find the point where that derivative is zero.  I.e., we want to find the set of $w_j$ where we have\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_j} = 0$$\n",
        "\n",
        "\n",
        "To figure out what this term is, we can apply the chain rule ($\\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx}$) multiple times to get the derivative of $L$ in terms of $p_i$, and the derivative of $p_i$ in terms of $w_j$. I.e.\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_j} = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial w_j}$$\n",
        "$$ = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial s_i}\\frac{\\partial s_i}{\\partial w_j}$$\n",
        "\n",
        "**Question 2.3**\n",
        "Using \"paper and pencil\" calculus, solve for each of the partial derivatives that make up the derivative of the loss function. Refer above for definitions of $L$, $p_i$, and $s_i$. Make sure to show your work.\n",
        "\n",
        "(a) Solve for the derivative of the Loss if $i\\in \\mathrm{Cat}$, i.e. $\\frac{\\partial L}{\\partial p_i}$\n",
        "\n",
        "(b) Solve for the derivative of the Loss if $i\\in \\mathrm{Jelly}$, i.e.$\\frac{\\partial L}{\\partial p_i}$\n",
        "\n",
        "(c) Solve for the derivative of the the sigmoid of the score, i.e. $\\frac{\\partial p_i}{\\partial s_i}$.  The derivative of the sigmoid is a special thing; normally you'd write the derivative as an expression terms of the input $s_i$, but the sigmoid derivative can be written as a very simple expression using only the output variable $p_i$.  Write $\\frac{\\partial p_i}{\\partial s_i}$ in that form, using only $p_i$.\n",
        "\n",
        "<!-- (d) Solve for the derivative of the score with respect to a weight $w_j$, i.e. $\\frac{\\partial s_i}{\\partial w_j}$ -->"
      ],
      "metadata": {
        "id": "xdk9zoUuOSI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.3 Answers:**\n",
        "\n",
        "(a) Edit Response Here\n",
        "\n",
        "(b) Edit Response Here\n",
        "\n",
        "(c) Edit Response Here\n",
        "\n",
        "<!-- (d) Edit Response Here -->"
      ],
      "metadata": {
        "id": "J-neArW8a67U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "Finally we want to know how the weight $w_i$ inflences the score $s_i$.\n",
        "\n",
        "$$\\frac{\\partial s_i}{\\partial w_j} = \\frac{\\partial }{\\partial w_j} \\sum_{j*} w_{j*} x_{ij*} = x_{ij}$$\n",
        "\n",
        "Remember that we were solving for this:\n",
        "$$\\frac{\\partial L}{\\partial w_j} = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial w_j}$$\n",
        "$$ = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial s_i}\\frac{\\partial s_i}{\\partial w_j}$$\n",
        "\n",
        "When we substitute in all the terms and simplify, we should get\n",
        "$$ = \\sum_{i \\in \\text{Cat}} (p_i - 1) x_{ij} + \\sum_{i \\in \\text{Jelly}} p_i  x_{ij} $$\n",
        "\n",
        "Now, let's define the prediction vector $p$ so that its $i$th component is $p_i$ and it contains the predicted probability for every sample in the data set. Let's also define the label vector $y$ so that $y_i = 1$ when $i$ is a Cat and zero otherwise.\n",
        "Finally, if we define the vector $x_j$ as the vector gathering together all the $x_{ij}$ for the same pixel position $j$, then we can write the derivative as a simple dot-product.\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_j} = (p - y) \\cdot x_j$$\n",
        "\n",
        "For the bias term, we can do the same set of calculations, and we get\n",
        "$$\\frac{\\partial L}{\\partial b} = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial b} = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial s_i} \\frac{\\partial s_i}{\\partial b} = \\sum_{i \\in \\text{Cat}} (p_i - 1) + \\sum_{i \\in \\text{Jelly}} p_i = \\sum_i (p-y) $$"
      ],
      "metadata": {
        "id": "tgZC7ilXO2-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Gradient Descent to Find Optimal Neuron Weights\n",
        "\n",
        "To summarize, the derivative of the loss function $L$ with respect to the parameters that we care about $w_j$, and $b$ are $\\frac{\\partial L}{\\partial w_j} = (p-y)^{T}x_j$, and $\\frac{\\partial L}{\\partial b} = \\sum_{i}({p-y})$. Remember that we want to find when these equations are equal to zero.\n",
        "\n",
        "\n",
        "It turns out that we cannot solve for a closed-form solution for $w$ and $b$. However, it is pretty easy to solve for using gradient descent if we use $\\frac{\\partial L}{\\partial w}$, and $\\frac{\\partial L}{\\partial b}$ as updates to our estimates of the weight and bias values.\n",
        "\n",
        "To do so, we can iterate through our dataset and compute $\\frac{\\partial L}{\\partial w}$, and $\\frac{\\partial L}{\\partial b}$. Then, we can update our weights using the following update rules:\n",
        "\n",
        "$$w_j = w_j - \\eta\\frac{\\partial L}{\\partial w_j}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$b = b - \\eta\\frac{\\partial L}{\\partial b}$$\n",
        "where $\\eta = 0.001$ is the step size.\n",
        "\n",
        "\n",
        "**Question 2.4**\n",
        "Implement code in `compute_weight_updates` to compute the weight and bias updates corresponding to $\\frac{\\partial L}{\\partial w}$, and $\\frac{\\partial L}{\\partial b}$. Then, run the gradient descent algorithm to test your implementation and see how well the neuron can classify cats vs. jellyfish."
      ],
      "metadata": {
        "id": "f9maeGxebldE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_weight_updates(neuron_weights, neuron_bias, dataset):\n",
        "    \"\"\"\n",
        "    Computes dL_dw and dL_db to solve for neuron weights via gradient descent.\n",
        "    \"\"\"\n",
        "    p_s = []\n",
        "    y_s = []\n",
        "    x_s = []\n",
        "\n",
        "    for x_i, y_i in ds:\n",
        "        p_s.append(None) # Replace None with appropriate code\n",
        "        y_s.append(None) # Replace None with appropriate code\n",
        "        x_s.append(None) # Replace None with appropriate code\n",
        "    p_minus_y = None\n",
        "\n",
        "    dL_dw = None # Replace with appropriate code\n",
        "    dL_db = None # Replace with appropriate code\n",
        "\n",
        "    return dL_dw, dL_db\n",
        "\n",
        "def find_neuron_weights(ds, dsv, n_iters=2000):\n",
        "    \"\"\"\n",
        "    Iterative Gradient Descent to optimize neuron weights\n",
        "    \"\"\"\n",
        "    neuron_weights, neuron_bias = init_neuron_weights()\n",
        "\n",
        "    for _ in range(n_iters):\n",
        "        if _ % 25 == 0:\n",
        "            print(f'Neg log likelihood {negative_logprob_of_dataset(ds, neuron_weights, neuron_bias)}')\n",
        "            print(f'Accuracy - train: {accuracy_of_data_set(ds, neuron_weights, neuron_bias)}, val: {accuracy_of_data_set(dsv, neuron_weights, neuron_bias)}')\n",
        "        dL_dw, dL_db = compute_weight_updates(neuron_weights, neuron_bias,ds)\n",
        "        neuron_weights -= 0.001 * dL_dw\n",
        "        neuron_bias -= 0.001 * dL_db\n",
        "\n",
        "    return neuron_weights, neuron_bias"
      ],
      "metadata": {
        "id": "-01AVg1fki4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run gradient descent\n",
        "final_weights, final_bias = find_neuron_weights(ds, dsv)"
      ],
      "metadata": {
        "id": "C3vhR773gANK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra Credit:**\n",
        "\n",
        "Even with 6x6 images, the optimization process to find the neuron weights can take quite a long time.\n",
        "\n",
        "There are several potential bottlenecks here. First, the dataloader used requires that we iterate through a single instance at a time. However, the dataset is small enough that it can be loaded into a single tensor.\n",
        "\n",
        "Extra Credit will be given for implementing ways to speed up the code above. For example, you might look to answer the following questions:\n",
        "\n",
        "- Can you re-implement the neuron weight-finding process so that you can do all the computations using only a few matrix multiplies and minimal for loops?\n",
        "- How much faster is the process if you use a GPU on top of these improvements?\n",
        "\n",
        "Please provide a writeup of your improvements, as well as any code you may have used below."
      ],
      "metadata": {
        "id": "p4u5KrOLy59G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7YIvzA3hl3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

