{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52290b6d",
      "metadata": {
        "id": "52290b6d"
      },
      "source": [
        "# Homework 2: Backpropagation\n",
        "\n",
        "In this first part of the homework, we will work with backpropagation.\n",
        "\n",
        "## Learning Objective\n",
        "\n",
        "In this part of the homework you will implement the core part of the backpropagation algorithm by hand.\n",
        "\n",
        "## Readings\n",
        "\n",
        "[Rumelhart, Hinton and Williams, *Learning representations\n",
        "by back-propagating errors*, 1986](https://papers.baulab.info/Rumelhart-1986.pdf) - Very influential in popularizing backpropagation, also known to numerical analysts as \"reverse mode autodifferentiation\".\n",
        "\n",
        "[Andreas Griewank, *On automatic differentiation and algorithmic linearization*, 2014](https://papers.baulab.info/also/Griewank-2014.pdf) - A mathematical presentation of both the forward and reverse-mode algorithm.  Since in deep network training we tpically have a single-dimensional scalar loss, we only focus on reverse-mode, which is far more efficient for our case.  May be helpful in the axioms and examples it provides.\n",
        "\n",
        "[Andreas Griewank, *Who invented the reverse mode of differentiation?*, 2012](https://papers.baulab.info/also/Griewank-2012.pdf) - Of historical interest.\n",
        "\n",
        "## Exercise 1: Implement backpropagation on paper\n",
        "\n",
        "Backpropagation is based on the idea of computing gradients efficently by remembering and reusing partial results.\n",
        "\n",
        "Consider the hyperbolic cosine function\n",
        "\n",
        "$$\n",
        "z = \\cosh{x} = \\frac{e^x + e^{-x}}{2}\n",
        "$$\n",
        "\n",
        "**Question 1.1**\n",
        "\n",
        "First, just compute $\\frac{dz}{dx}$ using ordinary calculus.  It turns out this is the sinh function:\n",
        "\n",
        "$$\n",
        "\\frac{dz}{dx} = \\sinh{x} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "Now, let's compute these numerically using backpropagation, done by hand.  Here is the computation graph for cosh:\n",
        "\n",
        "<img src=\"https://cs7150.baulab.info/2022-Fall/hw2/cosh-backprop.png\">\n",
        "\n",
        "**Question 1.2**\n",
        "\n",
        "First, fill in the forward pass.  Compute and fill in the following values in order.  You can write the answers in terms of `log` where it is cleaner, e.g., `log 0.5` or `log 2`, etc.   (Tip: it is much easier to work out the forward and backward pass by using a pencil and paper by drawing the graph and filling it out; then you can copy your answers here)\n",
        "\n",
        "$$\n",
        "a = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "c = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "d = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "e = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "z = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "**Question 1.3**\n",
        "\n",
        "Now, fill in the backward pass, in the reverse order.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial z} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial e} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial d} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial c} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial b} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial a} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x} = \\boxed{\\text{TODO: fill me in}}\n",
        "$$\n",
        "\n",
        "You can check your final answer for $\\frac{\\partial z}{\\partial x}$ against the value of sinh.  It should match.\n",
        "\n",
        "**Note**: The \"copy\" operation occurs frequently but is typically not explicitly represented as a computation node in code.  Instead, we see it appear as *gradient accumulation*, where multiple backward paths leading to the same node provide gradient that is summed and accumulated over all the paths.  That is the case in the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9797fc",
      "metadata": {
        "id": "bc9797fc"
      },
      "source": [
        "## Exercise 2: implement backpropagation in code\n",
        "\n",
        "The `ComputationNode` base class below provides a starting point for implementing the backpropagation algorithm.  A computation graph is created by connecting a set of `ComputationNodes`, culminating in a single output.  Every node records:\n",
        "\n",
        "* A list of any number of input edges connecting to input nodes, connected at construction (`self.input_nodes`)\n",
        "* A single output, where the result will be stored as a number computed in the forward pass (`self.result`)\n",
        "* A single gradient, which is the accumulated upstream gradient computed in the backward pass (`self.gradient`)\n",
        "\n",
        "Because a computation graph ends at a single output which is recursively connected to the result of the graph through the input edges, the whole graph can be represented by a reference to the output node.\n",
        "\n",
        "The `ComputationNode` base class itself does not know how to compute any specific mathematical operation.  Each separate math operation will need a subclass.  `Leaf`, `Negate`, `Exp`, and `Mean` are provided as examples.\n",
        "\n",
        "Each node also comes with convenience methods for traversing the computation graph that ends at the given node:\n",
        "\n",
        "* `sorted_nodes()` returns a list of all the computation nodes in toplogically sorted order, output last.\n",
        "* `get(name)` finds the single node matching the given name.\n",
        "\n",
        "In addition, similar to pytorch, the base class provides several convenience methods to compute the negation, exponent, etc, by constructing computation nodes that take the current node as input.\n",
        "\n",
        "Read and familiarize yourself with the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e4636e",
      "metadata": {
        "id": "52e4636e"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "import math\n",
        "\n",
        "class ComputationNode:\n",
        "    def __init__(self, *input_nodes, name=None):\n",
        "        '''\n",
        "        A computation graph is an acyclic graph of nodes, each representing\n",
        "        a value that is computed from its child (predecessor) input_nodes.\n",
        "        In our implementation, you must give the input nodes at construction time,\n",
        "        and we do not compute the result or the gradient right away.\n",
        "        That is your job.\n",
        "        '''\n",
        "        self.name = name\n",
        "        self.input_nodes = [n if isinstance(n, ComputationNode) else Leaf(n)\n",
        "                            for n in input_nodes]\n",
        "        self.result = None\n",
        "        self.gradient = None\n",
        "    def forward(self):\n",
        "        '''One step of the forward pass will fill in self.result.'''\n",
        "        assert 'Subclass implementation needed'\n",
        "        self.result = function_of(self.input(0))\n",
        "    def backward(self):\n",
        "        '''One step of the backward pass will combine upstream `self.gradient` with\n",
        "        \"local gradients\" to return a list of \"downstream gradients\", one for each input.'''\n",
        "        assert 'Subclass implementation needed'\n",
        "        return [compute_downstream_gradient(upstream_gradient, self.result, self.input(i))\n",
        "                for i in range(len(self.input_nodes))]\n",
        "    def accumulate_gradient(self, upstream_gradient):\n",
        "        '''Each downstream gradient should be accumulated as an upstream gradient.'''\n",
        "        if self.gradient is None:\n",
        "            self.gradient = upstream_gradient\n",
        "        else:\n",
        "            self.gradient = self.gradient + upstream_gradient\n",
        "    def sorted_nodes(self, seen=None):\n",
        "        '''\n",
        "        Returns a list of all the nodes in the computation graph, in topological\n",
        "        sort order, with all inputs listed before the outputs that depend on them.\n",
        "        '''\n",
        "        if seen is None: seen = set()\n",
        "        nodes = []\n",
        "        if self not in seen: # Do not process the same node twice\n",
        "            seen.add(self)\n",
        "            for node in self.input_nodes:\n",
        "                nodes.extend(node.sorted_nodes(seen=seen))\n",
        "            nodes.append(self)\n",
        "        return nodes\n",
        "    def input(self, i):\n",
        "        '''Gets the value of the ith input, after it is computed.'''\n",
        "        if _building_graph:\n",
        "            return self.input_nodes[i]\n",
        "        else:\n",
        "            return self.input_nodes[i].result\n",
        "    @property\n",
        "    def shape(self):\n",
        "        '''\n",
        "        Returns the shape of the result, if any.\n",
        "        '''\n",
        "        return getattr(self.result, 'shape', ())\n",
        "    def __repr__(self):\n",
        "        '''Print out the node and its inputs recursively so you can see the graph.'''\n",
        "        with printing_graph() as seen:\n",
        "            name = type(self).__name__ + (' ' + self.name if self.name else '')\n",
        "            if self in seen: return f'{name} @[{seen[self]}]'\n",
        "            else: seen[self] = str(len(seen) + 1)\n",
        "            our_repr = f'{name} result={self.result} gradient={self.gradient}'\n",
        "            tree_repr = '\\n'.join([our_repr] + list(repr(node) for node in self.input_nodes))\n",
        "            return '\\n  '.join(tree_repr.split('\\n'))\n",
        "\n",
        "    # Convenience methods.\n",
        "    def exp(self):\n",
        "        return Exp(self)\n",
        "    def sum(self):\n",
        "        return Matsum(self)\n",
        "    def expand(self, target_shape):\n",
        "        return Matexpand(self, target_shape)\n",
        "    def __neg__(self):\n",
        "        return Negate(self)\n",
        "    def __pow__(self, int_power):\n",
        "        return IntPow(self, int_power)\n",
        "    def __add__(self, other):\n",
        "        return Add(self, other) # You should make a Add class\n",
        "    def __radd__(self, other):\n",
        "        return Add(other, self) # You should make a Add class\n",
        "    def __sub__(self, other):\n",
        "        return Add(self, -other) # You should make a Add class\n",
        "    def __rsub__(self, other):\n",
        "        return Add(other, -self) # You should make a Add class\n",
        "    def __mul__(self, other):\n",
        "        return Mul(self, other) # You should make a Mul class\n",
        "    def __rmul__(self, other):\n",
        "        return Mul(other, self) # You should make a Mul class\n",
        "    def __truediv__(self, other):\n",
        "        return Mul(self, other ** -1) # You should make a Mul class\n",
        "    def __matmul__(self, other):\n",
        "        return Matmul(self, other) # You should make a Matmul class\n",
        "\n",
        "class Leaf(ComputationNode):\n",
        "    def __init__(self, value, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.value = value\n",
        "    def forward(self):\n",
        "        self.result = self.value\n",
        "    def backward(self):\n",
        "        return [] # no inputs, no downstream gradient\n",
        "\n",
        "class Negate(ComputationNode):\n",
        "    def forward(self):\n",
        "        self.result = -self.input(0)\n",
        "    def backward(self):\n",
        "        # return downstream gradient\n",
        "        return [-self.gradient]\n",
        "\n",
        "class Exp(ComputationNode):\n",
        "    def forward(self):\n",
        "        x = self.input(0)\n",
        "        self.result = x.exp() if callable(getattr(x, 'exp', None)) else math.exp(x)\n",
        "    def backward(self):\n",
        "        local_gradient = self.result\n",
        "        # return downstream gradient\n",
        "        return [local_gradient * self.gradient]\n",
        "\n",
        "class Mean(ComputationNode):\n",
        "    def forward(self):\n",
        "        self.result = (self.input(0) + self.input(1)) / 2\n",
        "    def backward(self):\n",
        "        # return downstream gradient for each of two inputs\n",
        "        return [self.gradient / 2, self.gradient / 2]\n",
        "\n",
        "class IntPow(ComputationNode):\n",
        "    def __init__(self, base, int_power):\n",
        "        super().__init__(base)\n",
        "        assert isinstance(int_power, int), 'Handles only constant integer powers'\n",
        "        # Note: you could rewrite this node to remove this restriction if you like.\n",
        "        self.int_power = int_power\n",
        "    def forward(self):\n",
        "        self.result = self.input(0) ** self.int_power\n",
        "    def backward(self):\n",
        "        # return downstream gradient\n",
        "        return [self.gradient * self.int_power * (self.input(0) ** (self.int_power - 1))]\n",
        "\n",
        "class Matsum(ComputationNode): # Support for tensor.sum() to reduce a tensor to a scalar\n",
        "    def forward(self):\n",
        "        self.result = self.input(0).sum()\n",
        "    def backward(self):\n",
        "        # return downstream gradient\n",
        "        return [self.gradient.expand(self.input(0).shape)]\n",
        "\n",
        "class Matexpand(ComputationNode): # Support for tensor.expand() to expand a scalar to a tensor\n",
        "    def __init__(self, inp, target_shape):\n",
        "        super().__init__(inp)\n",
        "        self.target_shape = target_shape\n",
        "    def forward(self):\n",
        "        self.result = self.input(0).expand(self.target_shape)\n",
        "    def backward(self):\n",
        "        # return downstream gradient\n",
        "        return [self.gradient.sum()]\n",
        "\n",
        "# This flag will be used for higher-order derivatives in an extra-credit part of this exercise.\n",
        "from contextlib import contextmanager\n",
        "_building_graph = False\n",
        "_printing_graph = None\n",
        "@contextmanager\n",
        "def build_graph(build_graph=True):\n",
        "    global _building_graph\n",
        "    old = _building_graph\n",
        "    _building_graph = build_graph\n",
        "    try:\n",
        "        yield old\n",
        "    finally:\n",
        "        _building_graph = old\n",
        "@contextmanager\n",
        "def printing_graph():\n",
        "    global _printing_graph\n",
        "    old = _printing_graph\n",
        "    if old is None:\n",
        "        _printing_graph = {}\n",
        "    try:\n",
        "        yield _printing_graph\n",
        "    finally:\n",
        "        _printing_graph = old"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ba36fd",
      "metadata": {
        "id": "d5ba36fd"
      },
      "source": [
        "The code below is not finished, and your job is to fill it in with a correct implementation of the backpropagation algorithm.\n",
        "\n",
        "Below we will give you six successively more difficult tests to pass, to get the code right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c3bbee",
      "metadata": {
        "id": "44c3bbee"
      },
      "outputs": [],
      "source": [
        "# MAIN CODE\n",
        "# TODO: you should modify and add code below to implement the backpropagation algorithm.\n",
        "# All the code you should need to add will be within these classes and functions.\n",
        "\n",
        "class Mul(ComputationNode):\n",
        "    def forward(self):\n",
        "        # TODO: Add your code here\n",
        "        self.result = 0\n",
        "    def backward(self):\n",
        "        # TODO: Add your code here\n",
        "        return []\n",
        "class Add(ComputationNode):\n",
        "    def forward(self):\n",
        "        # TODO: Add your code here\n",
        "        self.result = 0\n",
        "    def backward(self):\n",
        "        # TODO: Add your code here\n",
        "        return []\n",
        "class Matmul(ComputationNode):\n",
        "    def forward(self):\n",
        "        # TODO: Add your code here\n",
        "        self.result = 0\n",
        "    def backward(self):\n",
        "        # TODO: Add your code here\n",
        "        return []\n",
        "\n",
        "def forward_algorithm(graph):\n",
        "    # TODO: Add your code here\n",
        "    return graph\n",
        "\n",
        "def backward_algorithm(graph, one=1.0):\n",
        "    # TODO: Add your code here\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411a7572",
      "metadata": {
        "id": "411a7572"
      },
      "source": [
        "### Question 2.1, Compute the forward pass\n",
        "\n",
        "The following code constructs and prints the computation graph corresponding to the by-hand Exercise 1.  It only uses computation node classes that have already been implemented.\n",
        "\n",
        "It also runs the `forward_algorithm` which should compute the result of the formula.\n",
        "\n",
        "However, `forward_algorithm` is not yet implemented.\n",
        "\n",
        "Implement `forward_algorithm` in the `MAIN CODE` cell above, and then verify that the `sinh_x.result` printed in the sample below is correct.  Your code should pass through all the nodes in the graph in the appropriate order and invoke the `forward` method on them.\n",
        "\n",
        "Once you have your algorithm complete, verify that the `cosh_x.result` printed in the sample below is correct.\n",
        "\n",
        "**Note**: The convenient graph traversal functions noted above may be helpful for implementing `forward_algorithm` (and later `backward_algorithm` as well)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d0ec9a",
      "metadata": {
        "id": "14d0ec9a"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "x = Leaf(math.log(2), 'x')\n",
        "cosh_x = Mean(Exp(x), Exp(-x))\n",
        "print(forward_algorithm(cosh_x))\n",
        "print(cosh_x.result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e825e0",
      "metadata": {
        "id": "67e825e0"
      },
      "source": [
        "### **Question 2.2**, Compute the backward pass\n",
        "\n",
        "This is the tricky part.  Your job is to implement the `backward_algorithm` in the `MAIN CODE` cell above and then verify that the code below works.\n",
        "\n",
        "Remember that your backward algorithm should:\n",
        " * process all nodes in the reverse order as the `forward_algorithm`\n",
        " * start by placing 1.0 in the final node's gradient.\n",
        " * accumulate every downstream gradient as an upstream gradient for each coresponding input node.  You can use `input_node.accumulate_gradient(g)` to do this.\n",
        "\n",
        "Once you have your algorithm complete, verify that the `x.gradient` printed in the sample below is correct.  Note that the derivative of cosh is sinh, which can be used to tell you what answer you should be getting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf02466",
      "metadata": {
        "id": "6bf02466"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "print(backward_algorithm(cosh_x))\n",
        "print(x.gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce770621",
      "metadata": {
        "id": "ce770621"
      },
      "source": [
        "### **Question 2.3** - Implement Add and Mul, and verify things work with reused internal nodes\n",
        "\n",
        "The function tanh is defined as:\n",
        "\n",
        "$$z = \\tanh x = \\frac{\\sinh x}{\\cosh x} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
        "\n",
        "In the cell above, if you implement `Add` and `Mul`, you should be able to make the following test code work.  You should not need to change any of the code in this cell.\n",
        "\n",
        "Notice when you build the graph, the division $a/b$ is interpreted as $a * b^{-1}$, and the integer power $b{^-1}$ is already provided for you; you just need to provide `Mul`.  Similarly $a-b$ is interpreted as $a + (-b)$ and the negation $-b$ is already provided; you just need to provide `Add`.\n",
        "\n",
        "The final subtlety is that computation graph has been cleverly arranged to avoid re-computing `exp(x)` and `exp(-x)` more than necessary.  The resuse of these terms can lead to erroneous backpropagation if your algorithm re-visits nodes or edges too many times.  If the previous test worked but you are getting the wrong answer on this one, check that each node and edge in the graph is being visited exactly once.\n",
        "\n",
        "When are are done, verify that your result is correct.  Remember from class that $\\frac{dz}{dx} = 1 - z^2$ in the tanh case.  Check this result against the results of your code below, and if it does not match, fix your algorithm.  Once everything is right, you should not need to change anything in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5e9bf3",
      "metadata": {
        "id": "4c5e9bf3"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "x = Leaf(math.log(2), 'x')\n",
        "expx = Exp(x)\n",
        "expnx = Exp(-x)\n",
        "tanh_x = (expx - expnx) / (expx + expnx)\n",
        "forward_algorithm(tanh_x)\n",
        "backward_algorithm(tanh_x)\n",
        "print(tanh_x)\n",
        "print(x.gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e98062",
      "metadata": {
        "id": "b2e98062"
      },
      "source": [
        "### **Question 2.4** - Differentiate the following polynomial\n",
        "\n",
        "Now that you have `Add` and `Mul` you should be able to handle the polynomials like the one we differentiated in  HW1, but to make nice plces we will first need to handle tensors rather than just python scalars.\n",
        "\n",
        "To process gradients over tensors rather than just derivatives over python scalars you will need one other change.  Modify your `backward_algorithm` so that instead of starting at `1.0` it can start at `torch.tensor(1.0)`.  The code below uses the `one` argument to select the option that we want, passing in `one=torch.tensor(1.0)`, which the `backward_algorithm` implementation should use for the initial gradient.\n",
        "\n",
        "In principle, everything should then work.\n",
        "\n",
        "Note that we have already done a bit of the magic for you: the code in the first cell has already provided a completed implementation of `Matsum` which is used below when we say `yy.sum()`.\n",
        "\n",
        "Once you have your implementation above correct, you should not need to change the test code below; it should plot the polynomial and its first derivative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c49f43c",
      "metadata": {
        "id": "3c49f43c"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "import torch\n",
        "\n",
        "def polynomial(x):\n",
        "    return x**4 - 2 * x**3 + 3 * x**2 - 4 * x + 5\n",
        "\n",
        "xx = Leaf(torch.linspace(-2.0, 3.0, 25), 'x')\n",
        "yy = polynomial(xx)\n",
        "ysum = yy.sum()\n",
        "forward_algorithm(ysum)\n",
        "backward_algorithm(ysum, one=torch.tensor(1.0))\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(xx.value, yy.result, label='y')\n",
        "plt.plot(xx.value, xx.gradient, label='dy/dx')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e8af08e",
      "metadata": {
        "id": "0e8af08e"
      },
      "source": [
        "### **Question  2.5 (extra credit)** - Reproduce higher-order gradients\n",
        "\n",
        "Your code might already do everything needed to make the higher-order derviatives in the code below work, which just passes `one=Leaf(torch.tensor(1.0))` to do the trick of building a computation graph for the gradients.  But you may be missing some details.\n",
        "\n",
        "For extra credit, make higher-order derivatives work by getting those details right:\n",
        "\n",
        "  * You will need to zero the gradients in the whole tree at the beginning of your backward_algorithm, since the code will be repeating the backward algorithm.  Before starting its main work, your backward_algorithm can just pass over all the graph nodes and set `node.gradient = None`.\n",
        "  * Whenever you compute results, you will need to the ability to return computation graph nodes that can be further differentiated instead of plain numerical values.  You will want to do this when you are running code within the `with build_graph()` context manager.   Some code to do that has already been done in the provided `input` method in the code above; it may be enough, or you may need to add more adjustements to your code.\n",
        "  * You will need to make sure any methods you call on numbers or tensors also work on nodes.  Several of the provided methods have already been written to handle this case; it may be enough, but you may need to make adjustments.\n",
        "\n",
        "Without making significant changes to your implementation beyond the details above, the code below should work.\n",
        "\n",
        "You should not need to change any code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e380f1",
      "metadata": {
        "id": "d4e380f1"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# (extra credit)\n",
        "xx = Leaf(torch.linspace(-2.0, 3.0, 25), 'x')\n",
        "yy = polynomial(xx)\n",
        "ysum = yy.sum()\n",
        "forward_algorithm(ysum)\n",
        "plt.plot(xx.value, yy.result, label='y')\n",
        "\n",
        "with build_graph():\n",
        "    backward_algorithm(ysum, one=Leaf(torch.tensor(1.0)))\n",
        "dydx = xx.gradient\n",
        "dysum = dydx.sum()\n",
        "forward_algorithm(dysum)\n",
        "plt.plot(xx.value, dydx.result, label='dydx')\n",
        "\n",
        "with build_graph():\n",
        "    backward_algorithm(dysum, one=Leaf(torch.tensor(1.0)))\n",
        "d2yd2x = xx.gradient\n",
        "d2ysum = d2yd2x.sum()\n",
        "forward_algorithm(d2ysum)\n",
        "plt.plot(xx.value, d2yd2x.result, label='d2yd2x')\n",
        "\n",
        "with build_graph():\n",
        "    backward_algorithm(d2ysum, one=Leaf(torch.tensor(1.0)))\n",
        "d3yd3x = xx.gradient\n",
        "d3ysum = d3yd3x.sum()\n",
        "forward_algorithm(d3ysum)\n",
        "plt.plot(xx.value, d3yd3x.result, label='d3yd3x')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69da7f7f",
      "metadata": {
        "id": "69da7f7f"
      },
      "source": [
        "### **Question 2.6** - Make matrix multiplication work.\n",
        "\n",
        "Taking gradients with respect to matrix multiplications is the bread and butter of neural network training: it is what we spend most of our GPU cycles doing.\n",
        "\n",
        "Fortunately, the computation is very similar to taking gradients of scalar products: just as `Mul` multiplies the upstream gradient by swapped inputs, `Matmul` need to matrix-multiply the upstream gradient by swapped inputs.  However, you will need to get the order of the multiplications correct, and you will need to transpose the inputs in the appropriate way.\n",
        "\n",
        "\n",
        "A few hints:\n",
        "  * Gradients with respect to tensors should always be the same shape as the results.\n",
        "  * If you look online for advice, be aware that there are two different conventions for matrix calculus notation, one which is the transpose of the other, and if you mix them up then you can get the wrong answers.  (They are called numerator versus denominator notation; the Wikipedia entry on matrix calculus explains the differences well.)  Of the two, denominator notation is more natural for backpropagation.\n",
        "  * If you are unsure, the safest way to get the right matrix operation is to expand the partial derivatives for a particular entry of the matrix by hand, and then work out which matrix multiplication is equivalent to the sum of products.\n",
        "  * The easiest way to test that you have the right matrix operation is to test the code with nonrectangular inputs, and make sure that the shapes are compatible; then to check the results against pytorch.\n",
        "  * You may find this link helpful - https://en.wikipedia.org/wiki/Matrix_calculus\n",
        "\n",
        "Work through the linear algebra needed for computing partial derivatives of one or two specific entries of a matrix, and use your insights to implement `Matmul` in the `MAIN CODE` above by using matrix multiplications and transpose operations.  You should be able to pass the test below, which compares the gradients to the results given by pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7158ef2",
      "metadata": {
        "id": "e7158ef2"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "\n",
        "A, B, C = [Leaf(torch.tensor(d, requires_grad=True), name) for name, d in [\n",
        "  ('A', [[1.0, 1.0], [0.0, 1.0]]),\n",
        "  ('B', [[1.0, 2.0, 1.0], [0.0, 1.0, 1.0]]),\n",
        "  ('C', [[3.0, 0.0, 1.0], [1.0, 2.0, 3.0]]),\n",
        "]]\n",
        "loss = ((A @ B - C) ** 2).sum()\n",
        "\n",
        "# Use our algorithm, with pytorch autograd disabled during the backward pass.\n",
        "forward_algorithm(loss)\n",
        "with torch.no_grad():\n",
        "    backward_algorithm(loss, one=torch.tensor(1.0))\n",
        "\n",
        "# Use pytorch autograd.\n",
        "loss.result.backward()\n",
        "\n",
        "# Print and compare the results.\n",
        "import re\n",
        "def despace(s):\n",
        "    return re.sub(r\"\\s*\", \"\", str(s))\n",
        "for name, param in zip('ABC', [A, B, C]):\n",
        "    print(f'pytorch says: dloss/d{name} = {despace(param.value.grad)}')\n",
        "    print(f'our code says dloss/d{name} = {despace(param.gradient)}')\n",
        "    print(f'difference: {(param.value.grad - param.gradient).norm()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce2f584",
      "metadata": {
        "id": "2ce2f584"
      },
      "source": [
        "In practice, we will not implement backpropagation by hand.\n",
        "\n",
        "Instead, we will use the autograd functionality built in to pytorch.  In pytorch, the `Tensor` objects that have set `requires_grad=True` play the role of our `ComputationNode`.  Forward computations are done immediately, and backward passes are done using `tensor.backward()` or `autograd.grad()`.  Pytorch extends the backpropagation techniques we have explored here to cover backprop through every operation that it can do on a tensor."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}