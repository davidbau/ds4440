{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import random\n",
        "import math\n",
        "import string"
      ],
      "metadata": {
        "id": "eo1EwQhfQJ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Transformer\n",
        "\n",
        "First, let's look at an attention layer in detail.\n",
        "\n",
        "The job of the attention layer is to allow a token to \"see\" information from faraway long-ago tokens. It does this by forming a \"query\" vector that it uses to match other token \"key\" vectors. Then the information that it sees is put into a set of \"value\" vectors that are summed up and added to the original querying token state.\n",
        "\n",
        "This can all be done in parallel - let's look at how."
      ],
      "metadata": {
        "id": "kAZDzXd1-hZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        \"\"\"\n",
        "        Here we will assume that the input dimensions are same as the\n",
        "        output dims.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.q_layer = torch.nn.Linear(d, d)\n",
        "        self.k_layer = torch.nn.Linear(d, d)\n",
        "        self.v_layer = torch.nn.Linear(d, d)\n",
        "\n",
        "    def forward(self, x, mask=None, return_weights=False):\n",
        "        \"\"\"\n",
        "        Assume x is <t x d> -- t being the sequence length, d\n",
        "        the embed dims.\n",
        "\n",
        "        W_q, W_k, and W_v are weights for projecting into queries,\n",
        "        keys, and values, respectively. Here these will have shape\n",
        "        <d x t>, yielding d dimensional vectors for each input.\n",
        "\n",
        "        This function should return a t dimensional attention vector\n",
        "        for each input -- i.e., an attention matrix with shape <t x t>,\n",
        "        and the values derived from this <t x d>.\n",
        "\n",
        "        Derive Q, K, V matrices, then self attention weights. These should\n",
        "        be used to compute the final representations (t x d); optionally\n",
        "        return the weights matrix if `return_weights=True`.\n",
        "        \"\"\"\n",
        "        Q = self.q_layer(x)\n",
        "        K = self.k_layer(x)\n",
        "        V = self.v_layer(x)\n",
        "\n",
        "        A = Q @ K.transpose(-2, -1)\n",
        "        if mask is not None:\n",
        "            A = A.masked_fill(mask == 0, -1e9)\n",
        "        weights = F.softmax(A, dim=-1)\n",
        "\n",
        "        if return_weights:\n",
        "          return weights, weights @ V\n",
        "\n",
        "        return weights @ V"
      ],
      "metadata": {
        "id": "sgqYZmyO-muh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we put lots of attention layers into a transformer model.\n",
        "\n",
        "Around each attention layer we create a residual structure, and we also use LayerNorm to prevent things from blowing up.\n",
        "\n",
        "The last detail is Positional encoding - a vector we add to each token based on where it is, rather than what it is.  This allows a token to be \"queried\" according to position rather than content."
      ],
      "metadata": {
        "id": "vuRuwq1Q_Z4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
        "        self.layers = nn.ModuleList([SelfAttentionLayer(embed_dim) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.decoder = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        mask = torch.triu(torch.ones(\n",
        "            src.size(1), src.size(1), dtype=torch.bool, device=src.device)).T\n",
        "        src = self.embed(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        src = self.norm(src)\n",
        "        output = self.decoder(src)\n",
        "        return output\n",
        "\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.self_attn = SingleHeadAttention(embed_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        src = src + self.norm1(self.self_attn(src, mask=mask))\n",
        "        return src\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(100.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n"
      ],
      "metadata": {
        "id": "xEbCUxGivkmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data\n",
        "\n",
        "Here we make some training data."
      ],
      "metadata": {
        "id": "ZXEYDKss_2Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic dataset\n",
        "def generate_string_manipulation_data(num_examples):\n",
        "    examples = []\n",
        "    for _ in range(num_examples):\n",
        "        input_string = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 10)))\n",
        "        #instructions = random.choice(['reverse', 'upper', 'lower', 'capital', 'swap', 'echo'])\n",
        "        instructions = random.choice(['upper', 'echo'])\n",
        "\n",
        "        if instructions == 'reverse':\n",
        "            output_string = input_string[::-1]\n",
        "        elif instructions == 'upper':\n",
        "            output_string = input_string.upper()\n",
        "        elif instructions == 'lower':\n",
        "            output_string = input_string.lower()\n",
        "        elif instructions == 'capital':\n",
        "            output_string = input_string.capitalize()\n",
        "        elif instructions == 'swap':\n",
        "            output_string = input_string.swapcase()\n",
        "        elif instructions == 'echo':\n",
        "            output_string = input_string\n",
        "\n",
        "        examples.append(f\"{input_string} {instructions} {output_string}. \")\n",
        "\n",
        "    return examples\n",
        "\n",
        "print('\\n'.join(generate_string_manipulation_data(10)))"
      ],
      "metadata": {
        "id": "fMyseOOr6OzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally our special words \"upper\" and \"echo\" would just be a single token.\n",
        "We use a \"Byte Pair Encoding\" tokenizer to learn these rules based on our data."
      ],
      "metadata": {
        "id": "-P1jcjKY_6xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Split\n",
        "\n",
        "# Create a new CharBPETokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = Split(' ', 'isolated')\n",
        "\n",
        "# Train the tokenizer on the synthetic dataset\n",
        "trainer = BpeTrainer(vocab_size=100, min_frequency=2, special_tokens=[\"<unk>\"])\n",
        "tokenizer.train_from_iterator(generate_string_manipulation_data(1000), trainer=trainer)\n",
        "\n",
        "# Save the trained tokenizer\n",
        "tokenizer.save(\"char_bpe_tokenizer.json\")\n",
        "\n",
        "# Example usage\n",
        "encoded = tokenizer.encode(\"abcd capital .\")\n",
        "print(encoded.tokens)\n",
        "print(encoded.ids)"
      ],
      "metadata": {
        "id": "3NEyRlumQS8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below tokenizes batches of examples into tensors, for training."
      ],
      "metadata": {
        "id": "qZaoRCdGAHAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "decode = {v: k for k, v in vocab.items()}\n",
        "\n",
        "max_len = max([len(tokenizer.encode(s).ids) for s in generate_string_manipulation_data(10000)])\n",
        "\n",
        "def data_process(data):\n",
        "    global max_len\n",
        "    result = []\n",
        "    for item in data:\n",
        "        encoded = torch.zeros(max_len + 1, dtype=torch.long)\n",
        "        ids = tokenizer.encode(item).ids\n",
        "        encoded[:len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
        "        result.append(encoded)\n",
        "    return torch.stack(result)\n",
        "\n",
        "def make_epoch_data(batch_size, epoch_len):\n",
        "    flat_train_data = data_process(\n",
        "        generate_string_manipulation_data(batch_size * epoch_len)\n",
        "    )\n",
        "    return flat_train_data.view(epoch_len, batch_size, -1)\n",
        "\n",
        "make_epoch_data(2, 3)"
      ],
      "metadata": {
        "id": "cE9df07mZvyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The training loop\n",
        "\n",
        "Play with hyperparameters here"
      ],
      "metadata": {
        "id": "a5d6k08dAOq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 100\n",
        "seq_len = 100\n",
        "embed_dim = 32\n",
        "num_layers = 5\n",
        "num_epochs = 100\n",
        "warmup_learning_rate = 0.0002\n",
        "learning_rate = 0.002\n",
        "num_batches = 50\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = TransformerModel(len(vocab), embed_dim, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=warmup_learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_data = make_epoch_data(batch_size, num_batches).to(device)\n",
        "    for i in range(0, len(epoch_data)):\n",
        "        inputs = epoch_data[i,:,:-1]\n",
        "        targets = epoch_data[i,:,1:]\n",
        "        #print('inputs:', tokenizer.decode(inputs[0].tolist()))\n",
        "        #print('targets', tokenizer.decode(targets[0].tolist()))\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, len(vocab)), targets.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "    if epoch > 3:\n",
        "        optimizer.param_groups[0]['lr'] = learning_rate * (num_epochs - epoch) / num_epochs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YJoT-N2Tajks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "21Wu0E3SATKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_of_text = tokenizer.encode(\".\").ids[0]\n",
        "\n",
        "# Inference\n",
        "def generate_text(prompt, max_len=28):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt).ids\n",
        "    prompt_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output = model(prompt_tensor)\n",
        "            pred_token = output.argmax(dim=2)[:,-1]\n",
        "            if pred_token == 0:\n",
        "                break\n",
        "            prompt_tensor = torch.cat((prompt_tensor, pred_token.unsqueeze(0)), dim=1)\n",
        "\n",
        "    model.train()\n",
        "    predicted_sentence = ''.join([decode[t.item()] for t in prompt_tensor.squeeze()])\n",
        "    return predicted_sentence\n",
        "\n",
        "# Example usage\n",
        "prompt = \"amazing echo\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated Text: {generated_text}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ohvfq4PhanFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}