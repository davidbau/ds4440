{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Network Training\n",
        "\n",
        "Here we fashion and train a GAN to synthesize mnist digits."
      ],
      "metadata": {
        "id": "a82AYPKMWfj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bLTcDdE6W1Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A generator maps a vector to an image.\n",
        "\n",
        "We will use a simple convnet for that."
      ],
      "metadata": {
        "id": "hBXejbseW5UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "class Generator(nn.Sequential):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__(\n",
        "            nn.ConvTranspose2d(latent_dim, 128, 7, 1, 0),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "latent_dim = 100\n",
        "generator = Generator(latent_dim)\n",
        "\n",
        "# Sample some random noise\n",
        "z_samples = torch.randn(12, latent_dim, 1, 1)\n",
        "\n",
        "# Plot some samples\n",
        "def show_samples():\n",
        "    with torch.no_grad():\n",
        "        fake_data = generator(z_samples).reshape(12, 28, 28).numpy()\n",
        "        fig, axs = plt.subplots(1, 12, figsize=(11, 1))\n",
        "        for i in range(12):\n",
        "            axs[i].imshow(fake_data[i], cmap='gray')\n",
        "            axs[i].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "show_samples()"
      ],
      "metadata": {
        "id": "7_m8wuOlXIJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A discriminator is a classifier\n",
        "\n",
        "This is a simple convnet, with a single (binary) class output prediction."
      ],
      "metadata": {
        "id": "LeakIgW9Xdex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator\n",
        "class Discriminator(nn.Sequential):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__(\n",
        "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 1, 7, 1, 0, bias=False)\n",
        "        )"
      ],
      "metadata": {
        "id": "0PCd7-1yar2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The GAN training loop\n",
        "\n",
        "There are a few tricks here.\n",
        " * How many optimizers are there. Why?\n",
        " * On each iteration, how many times is the generator run?\n",
        " * How many times is the discriminator run?  What is the purpose of each run?"
      ],
      "metadata": {
        "id": "L8FUKghudIne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDQlmtp-KHBs"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator(latent_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Loss functions and optimizers\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    show_samples()\n",
        "\n",
        "    for batch_idx, (real_data, _) in enumerate(dataloader):\n",
        "        # Train discriminator\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        real_output = discriminator(real_data)\n",
        "\n",
        "        z = torch.randn(real_data.size(0), latent_dim, 1, 1)\n",
        "        fake_data = generator(z)\n",
        "        fake_output = discriminator(fake_data.detach())\n",
        "\n",
        "        real_labels = torch.ones_like(real_output)\n",
        "        fake_labels = torch.zeros_like(fake_output)\n",
        "        real_loss = criterion(real_output, real_labels)\n",
        "        fake_loss = criterion(fake_output, fake_labels)\n",
        "        d_loss = real_loss + fake_loss\n",
        "\n",
        "        d_loss.backward(retain_graph=True)  # Set retain_graph=True here\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train generator\n",
        "        generator.zero_grad()\n",
        "        fake_output = discriminator(fake_data)\n",
        "\n",
        "        real_labels = torch.ones_like(fake_output)\n",
        "        g_loss = criterion(fake_output, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    # Print losses and visualize generator output\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}\")\n",
        "\n"
      ]
    }
  ]
}
