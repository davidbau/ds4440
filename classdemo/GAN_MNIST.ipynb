{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Network Training\n",
        "\n",
        "Here we fashion and train a GAN to synthesize mnist digits."
      ],
      "metadata": {
        "id": "a82AYPKMWfj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bLTcDdE6W1Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A generator maps a vector to an image.\n",
        "\n",
        "We will use a simple convnet for the generator.\n",
        "\n",
        "* It takes a 100-dimensional random input and produces an image.\n",
        "* At the start it produces a random garbage image for any input vector."
      ],
      "metadata": {
        "id": "hBXejbseW5UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "class Generator(nn.Sequential):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__(\n",
        "            nn.ConvTranspose2d(latent_dim, 128, 7, 1, 0),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "latent_dim = 100\n",
        "generator = Generator(latent_dim)\n",
        "\n",
        "# Sample some random noise\n",
        "z_samples = torch.randn(12, latent_dim, 1, 1)\n",
        "\n",
        "# Plot some samples\n",
        "def show_samples():\n",
        "    with torch.no_grad():\n",
        "        fake_data = generator(z_samples).reshape(12, 28, 28).numpy()\n",
        "        fig, axs = plt.subplots(1, 12, figsize=(11, 1))\n",
        "        for i in range(12):\n",
        "            axs[i].imshow(fake_data[i], cmap='gray')\n",
        "            axs[i].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "show_samples()"
      ],
      "metadata": {
        "id": "7_m8wuOlXIJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A discriminator is a classifier\n",
        "\n",
        "This is a simple convnet, with a single (binary) class output prediction.\n",
        "\n",
        "* It takes an image as input.\n",
        "* And then it produces a logit as output.\n",
        "\n",
        "(It will still need to be softmaxed or passed through a sigmoid to be a classification probability. We will do that later, in the loss by using `BCEWithLogitsLoss` which does binary cross-entropy loss a built-in sigmoid to preprocess the logits into probabilities.)"
      ],
      "metadata": {
        "id": "LeakIgW9Xdex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator\n",
        "class Discriminator(nn.Sequential):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__(\n",
        "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 1, 7, 1, 0, bias=False)\n",
        "        )"
      ],
      "metadata": {
        "id": "0PCd7-1yar2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data"
      ],
      "metadata": {
        "id": "1DxTpTMgYazS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Show a sample of images\n",
        "fig, axs = plt.subplots(1, 12, figsize=(11, 1))\n",
        "for i in range(12):\n",
        "    axs[i].imshow(dataset[i][0].squeeze().numpy(), cmap='gray')\n",
        "    axs[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jhYi5O8WYYvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The GAN training loop\n",
        "\n",
        "There are a few tricks here. Some questions to think about.\n",
        " * How many optimizers are there. Why?\n",
        " * On each iteration, how many times is the generator run?\n",
        " * How many times is the discriminator run?  What is the purpose of each run?\n",
        "\n"
      ],
      "metadata": {
        "id": "L8FUKghudIne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDQlmtp-KHBs"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator(latent_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Loss functions and optimizers\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    show_samples()\n",
        "\n",
        "    for batch_idx, (real_data, _) in enumerate(dataloader):\n",
        "        # Train discriminator\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        # The true label for real data is 1\n",
        "        real_estimate = discriminator(real_data)\n",
        "        real_labels = torch.ones_like(real_estimate)\n",
        "        real_loss = criterion(real_estimate, real_labels)\n",
        "\n",
        "        # The true label for fake data is 0\n",
        "        z = torch.randn(real_data.size(0), latent_dim, 1, 1)\n",
        "        fake_data = generator(z)\n",
        "        fake_estimate = discriminator(fake_data.detach())\n",
        "        fake_labels = torch.zeros_like(fake_estimate)\n",
        "        fake_loss = criterion(fake_estimate, fake_labels)\n",
        "\n",
        "        d_loss = real_loss + fake_loss\n",
        "\n",
        "        d_loss.backward(retain_graph=True)  # Set retain_graph=True here\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train generator\n",
        "        generator.zero_grad()\n",
        "\n",
        "        # For the generator, the target label for fake data is 1\n",
        "        fake_estimate_to_beat = discriminator(fake_data)\n",
        "        real_labels = torch.ones_like(fake_estimate_to_beat)\n",
        "        g_loss = criterion(fake_estimate_to_beat, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    # Print losses and visualize generator output\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus: Wasserstein GAN\n",
        "\n",
        "The following is a variant of the Wasserstein GAN. Instead of using cross-entropy loss which saturates because it works in probabilities, it directly guides the discriminator to drive fake_estimate down towards negative infinity and real_estimate high towards positive infinity by minimizing (fake_estimate - real_estimate).\n",
        "\n",
        "Even though it avoids saturation by letting the discriminator predict scores outside [0,1], it would explode if left unconstrained. The Wasserstein-GP scheme avoids explosions in the discriminator by severely penalizing gradients larger than one along the path between real and fake images. In the code below, that is `gradient_penalty`.\n",
        "\n",
        "Using this discriminator, Wasserstein GAN training directly guides the generator to maximize generate images that maximize `fake_estimate_to_beat` (making its fake images look as real as possible)."
      ],
      "metadata": {
        "id": "wWQVuHXrVYWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize generator and discriminator\n",
        "generator = Generator(latent_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    show_samples()\n",
        "\n",
        "    for batch_idx, (real_data, _) in enumerate(dataloader):\n",
        "        # Train discriminator\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        real_estimate = discriminator(real_data)\n",
        "\n",
        "        z = torch.randn(real_data.size(0), latent_dim, 1, 1)\n",
        "        fake_data = generator(z)\n",
        "        detached_fake_data = fake_data.detach()\n",
        "        fake_estimate = discriminator(detached_fake_data)\n",
        "\n",
        "        # Gradient penalty\n",
        "        alpha = torch.rand(real_data.size(0), 1, 1, 1).to(real_data.device)\n",
        "        interpolated = alpha * real_data + (1 - alpha) * detached_fake_data\n",
        "        interpolated.requires_grad = True\n",
        "        interpolated_estimate = discriminator(interpolated)\n",
        "        grad_outputs = torch.ones_like(interpolated_estimate)\n",
        "        gradients = torch.autograd.grad(outputs=interpolated_estimate, inputs=interpolated,\n",
        "                                        grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
        "        gradients = gradients.view(gradients.size(0), -1)\n",
        "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "        # WGAN-GP loss: fakes should be very negative and reals very positive\n",
        "        # BUT the discriminator should not have gradients sloped more than 1.\n",
        "        d_loss = fake_estimate.mean() - real_estimate.mean() + 20 * gradient_penalty\n",
        "\n",
        "        d_loss.backward(retain_graph=True)\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train generator\n",
        "        generator.zero_grad()\n",
        "\n",
        "        # WGAN-GP generator loss: generator wants to make the estimate positive\n",
        "        z = torch.randn(real_data.size(0), latent_dim, 1, 1)\n",
        "        fake_estimate_to_beat = discriminator(fake_data)\n",
        "        g_loss = -fake_estimate_to_beat.mean()\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    # Print losses and visualize generator output\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "Mx0Km4CgVW7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24XgSnDfXhCe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}