{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient scales\n",
        "\n",
        "In this notebook we will take a look at a simple training setting.\n",
        "\n",
        "And then we will confront one of the big challenges that faced neural network practitioners for 20 years: the challenge of gradient scaling.\n",
        "\n",
        "* If derivatives are too large, then you get explosions when doing gradient descent.\n",
        "* If derivatives are too small, then your optimizer goes nowhere.\n",
        "* And the worst case is when the derivatives are both small and large in the same system.\n",
        "\n"
      ],
      "metadata": {
        "id": "8aN2RUTdQcva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show set-up code"
      ],
      "metadata": {
        "id": "mCmrG2zthGng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This visulaization function is used for the next example\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def deviceof(net):\n",
        "    return next(net.parameters()).device\n",
        "\n",
        "def visualize_net(net, classify_target):\n",
        "    fig, axd = plt.subplot_mosaic(\"AB\\nAB\\nCC\")\n",
        "    ax1, ax2, ax3 = [axd[k] for k in \"ABC\"]\n",
        "    if net:\n",
        "        grad_hist(net, ax3)\n",
        "    grid = torch.stack([\n",
        "        torch.linspace(-2, 2, 100)[None, :].expand(100, 100),\n",
        "        torch.linspace(2, -2, 100)[:, None].expand(100, 100),\n",
        "    ])\n",
        "    x, y = grid\n",
        "    target = classify_target(x, y)\n",
        "    ax1.set_title('target')\n",
        "    ax1.imshow(target.float(), cmap='hot', extent=[-2,2,-2,2])\n",
        "    if net:\n",
        "        ax2.set_title('network output')\n",
        "        score = net(grid.permute(1, 2, 0).reshape(-1, 2).to(deviceof(net))).softmax(1)\n",
        "        ax2.imshow(score[:,1].reshape(100, 100).detach().cpu(), cmap='hot', extent=[-2,2,-2,2])\n",
        "    plt.show()\n",
        "\n",
        "# Function to draw a histogram of all the gradients\n",
        "def grad_hist(net, ax):\n",
        "    all_grads = []\n",
        "    for p in net.parameters():\n",
        "        if p.grad is not None:\n",
        "            all_grads.append(p.grad.detach().cpu().flatten())\n",
        "    all_grads = torch.cat(all_grads).abs().clamp(1e-9)\n",
        "    _, bins = np.histogram(np.log10(all_grads), bins='auto')\n",
        "    ax.hist(all_grads, bins=10**bins)\n",
        "    ax.set_xscale('log')"
      ],
      "metadata": {
        "id": "E6uPiwwRhENn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a three-layer MLP\n",
        "\n",
        "We'll start with a defining a neural network.\n"
      ],
      "metadata": {
        "id": "iTFsIqQ0hO1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jONKBm58gAho",
        "outputId": "7f9b9882-c8fe-405e-f9d6-96d889f8d984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=20, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=20, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch, math\n",
        "from torch.nn.functional import cross_entropy\n",
        "from collections import OrderedDict\n",
        "from torch.nn import Linear, ReLU, Sequential\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "\n",
        "# Defining a three-layer MLP.\n",
        "mlp = torch.nn.Sequential(OrderedDict([\n",
        "    ('layer1', Sequential(Linear(2, 20), ReLU())),\n",
        "    ('layer2', Sequential(Linear(20, 20), ReLU())),\n",
        "    ('layer3', Sequential(Linear(20, 2)))\n",
        "]))\n",
        "\n",
        "print(mlp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the training set\n",
        "\n",
        "Here we define a toy problem: given a point (x, y), define whether it is in class 0 or 1 using a simple (but nonlinear) mathematical rule."
      ],
      "metadata": {
        "id": "h1qqUeExiVf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup: define the training set.\n",
        "def classify_target(x, y):\n",
        "    return (y > (x * 3).sin()).long()\n",
        "\n",
        "visualize_net(None, classify_target)"
      ],
      "metadata": {
        "id": "-H2OjE0DiT1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The four parts of a training loop"
      ],
      "metadata": {
        "id": "FbylYXrIhTkM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSaZRBoAHpqs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import printable\n",
        "def ocassional(iteration):\n",
        "    return iteration == 2 ** iteration.bit_length() - 1\n",
        "\n",
        "# initialize the network somehow\n",
        "for i, p in enumerate(mlp.parameters()):\n",
        "    p.data[...] = torch.linspace(0, i % 3 - 1, p.numel()).reshape(p.shape)\n",
        "\n",
        "optimizer = SGD(mlp.parameters(), lr=0.01)\n",
        "for iteration in range(1024):\n",
        "\n",
        "    # Step 1: sample the training data\n",
        "    in_batch = torch.randn(10000, 2, device=deviceof(mlp))\n",
        "    target_batch = classify_target(in_batch[:,0], in_batch[:,1])\n",
        "\n",
        "    # Step 2: compute the loss\n",
        "    out_batch = mlp(in_batch)\n",
        "    loss = cross_entropy(out_batch, target_batch)\n",
        "\n",
        "    # Step 3: update the weights\n",
        "    mlp.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Step 4: test performance on different data\n",
        "    if ocassional(iteration):\n",
        "        with torch.no_grad():\n",
        "            visualize_net(mlp, classify_target)\n",
        "            pred_batch = out_batch.max(1)[1]\n",
        "            accuracy = (pred_batch == target_batch).float().sum() / len(in_batch)\n",
        "            print(f'Iteration {iteration} accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "3WyWSJXkgJNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization: exercises to do\n",
        "\n",
        "Try changing the weight initialization rule above.  Can we make it behave better?"
      ],
      "metadata": {
        "id": "3oudPjV6_02C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Momentum and ADAM: Exercises to do.\n",
        "\n",
        "1. Implement our own GD.\n",
        "2. Write a simple EMA function.\n",
        "3. Track an EMA of the gradient.\n",
        "4. Track and plot EMA of RMS.\n",
        "5. Try to put it together as ADAM.\n",
        "6. Fix the EMA function."
      ],
      "metadata": {
        "id": "sACGtJkf83z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(my_optimizer):\n",
        "    def ocassional(iteration):\n",
        "        return iteration == 2 ** iteration.bit_length() - 1\n",
        "\n",
        "    # initialize the network somehow\n",
        "    for i, p in enumerate(mlp.parameters()):\n",
        "        p.data[...] = torch.linspace(0, i % 3 - 1, p.numel()).reshape(p.shape)\n",
        "\n",
        "    optimizer = my_optimizer(mlp.parameters(), lr=0.01)\n",
        "    for iteration in range(1024):\n",
        "\n",
        "        # Step 1: sample the training data\n",
        "        in_batch = torch.randn(10000, 2, device=deviceof(mlp))\n",
        "        target_batch = classify_target(in_batch[:,0], in_batch[:,1])\n",
        "\n",
        "        # Step 2: compute the loss\n",
        "        out_batch = mlp(in_batch)\n",
        "        loss = cross_entropy(out_batch, target_batch)\n",
        "\n",
        "        # Step 3: update the weights\n",
        "        mlp.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step 4: test performance on different data\n",
        "        if ocassional(iteration):\n",
        "            with torch.no_grad():\n",
        "                visualize_net(mlp, classify_target)\n",
        "                pred_batch = out_batch.max(1)[1]\n",
        "                accuracy = (pred_batch == target_batch).float().sum() / len(in_batch)\n",
        "                print(f'Iteration {iteration} accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "bGXYWPi89m0I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyOptimizer:\n",
        "    def __init__(self, params, lr):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        for p in self.params:\n",
        "            p.data -= self.lr * p.grad\n",
        "\n",
        "run_training(MyOptimizer)"
      ],
      "metadata": {
        "id": "I-JAMm6q_gE8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}