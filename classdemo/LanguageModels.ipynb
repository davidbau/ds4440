{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Language Models\n",
        "\n",
        "In this notebook, we introduce language models. We start with a classic ngram (trigram) language model, and then we load up a state-of-the-art pretrained transformer neural language model, and we use it in the same way.\n",
        "\n",
        "## Building an ngram model.\n",
        "\n",
        "Let's construct an ngram model.  We will build and \"train\" the ngram model from scratch, from some training text that we'll load from the Gutenberg public-domain text dataset."
      ],
      "metadata": {
        "id": "-MDCvjyacYhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "import random\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Download and load a sample text dataset\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "id": "aNIXQMXiZjHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this API, you could load, e.g. Shakespere's Hamlet by saying `nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')`  (Try it!) Below we load Jane Austen's Emma.\n",
        "\n",
        "Then we print some of the text."
      ],
      "metadata": {
        "id": "3s5BgEDcdVDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "print(f'The text is {len(text)} characters long. Here is the beginning:\\n{text[:300]}')"
      ],
      "metadata": {
        "id": "odc7oqPndP-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "An ngram model can be built on letters or words or other chunks of text.\n",
        "\n",
        "Here we will split the text into words.\n",
        "\n",
        "If you'd like to create a character-based ngram model, you can try `tokens = list(text)` instead."
      ],
      "metadata": {
        "id": "jC8aAY_-ddDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "tokens = nltk.word_tokenize(text.lower())\n"
      ],
      "metadata": {
        "id": "_lLf-OzFdY20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count trigram statistics\n",
        "\n",
        "The trigrams function chops up text into a list of triples of adjacent words, like this:"
      ],
      "metadata": {
        "id": "d3HdotCTfqwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(trigrams(tokens))[:10]"
      ],
      "metadata": {
        "id": "b8GztizmfiPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run through every triple of adjacent tokens."
      ],
      "metadata": {
        "id": "6sHcmptJgPPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build trigram statistics\n",
        "trigram_model = defaultdict(Counter)\n",
        "for w1, w2, w3 in trigrams(tokens, pad_right=True, pad_left=True):\n",
        "    trigram_model[(w1, w2)][w3] += 1\n",
        "\n",
        "# Convert counts to probabilities\n",
        "for w1_w2 in trigram_model:\n",
        "    total_count = float(sum(trigram_model[w1_w2].values()))\n",
        "    for w3 in trigram_model[w1_w2]:\n",
        "        trigram_model[w1_w2][w3] /= total_count\n",
        "\n"
      ],
      "metadata": {
        "id": "BOCW9nB3cxcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining the model\n",
        "\n",
        "We can read the trigram model by supplying any bigram as input; then it gives us the probability distribution of the next words.\n",
        "\n",
        "We often write this as\n",
        "$$p(y | x)$$\n",
        "where $x$ is the sequence of previous words, in this case, $x = ['she', 'saw']$, and $y$ is the next word to be predicted, which will be selected based on the table."
      ],
      "metadata": {
        "id": "teEX6km8gwen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model[('she', 'saw')]"
      ],
      "metadata": {
        "id": "TvcDdVgtgjS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text with a trigram model\n",
        "\n",
        "We can repeat the process of choosing a next word based on this rule."
      ],
      "metadata": {
        "id": "bT9TJhZ_gVnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling function to generate text\n",
        "text = ['she', 'saw']\n",
        "for _ in range(50):\n",
        "    next_words = list(trigram_model[tuple(text[-2:])].keys())\n",
        "    if not next_words:  # Check if there are no possible next words\n",
        "        break\n",
        "    next_word = random.choices(next_words, weights=trigram_model[tuple(text[-2:])].values())[0]\n",
        "    if next_word is None:  # End of sentence\n",
        "        break\n",
        "    text.append(next_word)\n",
        "print(' '.join(text))\n"
      ],
      "metadata": {
        "id": "chVyr6YOgTTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a close look at the output.\n",
        "\n",
        "Try running it a few times.\n",
        "\n",
        "What do you think of the generated text?"
      ],
      "metadata": {
        "id": "091BTsr8Fu_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Language Modeling"
      ],
      "metadata": {
        "id": "UZhEDILR6edM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load a pretrained transformer called \"distilgpt2\".\n",
        "\n",
        "Instead of looking up probabilities in a table, it uses a neural network to estimate probabilities. This neural network was pretrained on a large amount of web text.\n",
        "\n",
        "The neural network is a classifier, and it needs to have a fixed vocabulary of tokens to choose between for each \"word\".  So we use a tokenizer algorithm that splits text using a fixed vocabulary of about 50 thousand tokens."
      ],
      "metadata": {
        "id": "iI-0_2KN-yfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")"
      ],
      "metadata": {
        "id": "Da0kVnO-cAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the tokenizer object\n",
        "\n",
        "The GPT2 tokenizer splits a string into word-like tokens and translates each of them to a number.  See how the number 2497 corresponds  to the token `' saw'`."
      ],
      "metadata": {
        "id": "h-WjFRNe_JkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the initial prompt\n",
        "prompt = \"She saw\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "print(input_ids)\n",
        "print([tokenizer.decode(input_ids[0, i]) for i in range(len(input_ids[0]))])"
      ],
      "metadata": {
        "id": "1U2pp6fx_JEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining raw transformer model outputs\n",
        "\n",
        "A transformer `model` is a neural network that takes a sequence of tokens as inputs and produces a grid of numbers, `logits`, as outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "pYlrfSzR-vKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_outputs = model(input_ids).logits\n",
        "print(transformer_outputs.shape)\n",
        "print(transformer_outputs[0, :, :6])\n"
      ],
      "metadata": {
        "id": "fLr4ZOi-7fF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying softmax and examimining probabilities\n",
        "\n",
        "The each row of numbers are raw neural network classifier outputs, for a 50257-way classifier. To convert to probabilities, we put them through a softmax.\n",
        "\n",
        "Each row of classifier output predicts the token following each of the tokens in the array (since we provided two tokens, we get two predictions). To generate, we are just interested in the last row.\n",
        "\n",
        "The following function gets these probabilities; we run it and print out the top 10 probabilities."
      ],
      "metadata": {
        "id": "720N6wQxAgpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_probabilities(input_ids):\n",
        "    transformer_outputs = model(input_ids).logits\n",
        "    probabilities = torch.nn.functional.softmax(transformer_outputs, dim=-1)\n",
        "    return probabilities[:, -1, :]\n",
        "\n",
        "probabilities = get_probabilities(input_ids)\n",
        "\n",
        "print(f\"after \\\"{tokenizer.decode(input_ids[0])}\\\":\")\n",
        "for v, k in zip(*probabilities[0].topk(10)):\n",
        "    print(f\"{tokenizer.decode([k])}: {v.item():.3f}\")"
      ],
      "metadata": {
        "id": "F-JD5TBJ8l_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating text\n",
        "\n",
        "We can iteratively sample from the neural language model to generate text."
      ],
      "metadata": {
        "id": "C3Y5mAewEx3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = input_ids\n",
        "\n",
        "# Generation loop\n",
        "for _ in range(50):\n",
        "    # Get the logits for the last token\n",
        "    probabilities = get_probabilities(output_ids)\n",
        "    next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "    # Append the sampled token to the output_ids\n",
        "    output_ids = torch.cat([output_ids, next_token], dim=-1)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "Hcr9aUxt7fVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a close look at the output from the neural language model.\n",
        "\n",
        "Try running it a few times.\n",
        "\n",
        "What do you think of the generated text?\n",
        "\n",
        "How does it compare to the trigram model output?\n"
      ],
      "metadata": {
        "id": "L7_meMv5E-DW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking inside the transformer model\n",
        "\n",
        "Let's look inside the neural language model. What is inside a transformer?"
      ],
      "metadata": {
        "id": "F-etMx4xHNCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "8LQFBOUdHTjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understaning an encoder layer.\n",
        "\n",
        "The first thing to understand is that the transformer uses an *encoder* to translate each input token into a vector. The encoder layer is called `model.transformer.wte`.\n",
        "\n",
        "Let's use the encoder to encode the two tokens 'She saw`:"
      ],
      "metadata": {
        "id": "li1Anjd_HxJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.transformer.wte(input_ids)\n",
        "print(embeddings)\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "bBfcc5ttHmc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the embeddings are 768-dimensional vectors. How does this compare to the vocabulary size?\n",
        "\n",
        "## Inverting the embedding matrix\n",
        "\n",
        "We can transpose the embedding matrix to get an approximate inversion that will tell us \"which token has the embedding closest to a vector\".\n",
        "\n",
        "Here we will get the embedding vectors for Paris, France, and Tokyo, and invert them back to the original tokens."
      ],
      "metadata": {
        "id": "C2ojxROiIIXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids = tokenizer.encode(\" Paris France Tokyo\", return_tensors=\"pt\")\n",
        "embeddings = model.transformer.wte(test_ids)\n",
        "inverse_embeddings = model.transformer.wte.weight.t()\n",
        "print(embeddings)\n",
        "print(embeddings.shape)\n",
        "\n",
        "\n",
        "scores = embeddings @ inverse_embeddings\n",
        "for v, k in zip(*scores[0].max(dim=-1)):\n",
        "    print(f\"{tokenizer.decode([k])}: {v.item():.3f}\")"
      ],
      "metadata": {
        "id": "sY7arMs2IR7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic vector algebra\n",
        "\n",
        "What should we get if we calculate the vector \"Tokyo - Paris + France\"?\n",
        "\n",
        "Try it below.  The code prints the 10 tokens with the closest embeddings.\n",
        "\n",
        "These type of vector algebra relationships emerge naturally when the embedding vectors are learned using neural network training."
      ],
      "metadata": {
        "id": "WHAH4NvIPNA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec = embeddings[0, 2, :] - embeddings[0, 0, :] + embeddings[0, 1, :]\n",
        "scores = vec[None] @ inverse_embeddings\n",
        "for v, k in zip(*scores[0].topk(10)):\n",
        "    print(f\"{tokenizer.decode([k])}: {v.item():.3f}\")"
      ],
      "metadata": {
        "id": "BJ4gOeBbL7_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}