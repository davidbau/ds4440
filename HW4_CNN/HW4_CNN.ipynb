{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidbau/ds4440/blob/main/HW4_CNN/HW4_CNN.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Objectives\n",
    "* Understand the functionality of a filter by manually creating one\n",
    "* Understand different hyperparameters of Conv2D layer\n",
    "* Understand how these hparams affect the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! DO NOT CHANGE THIS CELL !\n",
    "\n",
    "import torch\n",
    "umbrella = [\n",
    "    [-1, -1, 1, -1, -1],\n",
    "    [-1, 1, 1, 1, -1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [-1, -1, 1, -1, -1],\n",
    "    [-1, -1, 1, -1, -1],\n",
    "]\n",
    "plus = [\n",
    "    [-1, -1, 1, -1, -1],\n",
    "    [-1, -1, 1, -1, -1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [-1, -1, 1, -1, -1],\n",
    "    [-1, -1, 1, -1, -1],\n",
    "]\n",
    "\n",
    "plus = torch.tensor(plus)\n",
    "umbrella = torch.tensor(umbrella)\n",
    "\n",
    "filled = torch.ones(5, 5)\n",
    "hollow = torch.zeros(5, 5) - 1\n",
    "hollow[0][1:-1] += 2\n",
    "hollow[-1][1:-1] += 2\n",
    "hollow[:, 0] += 2\n",
    "hollow[:, -1] += 2\n",
    "\n",
    "patterns = [umbrella]*5 + [plus, filled, hollow, plus]  \n",
    "\n",
    "for rot in [1, 1, 2, 2, 3]:\n",
    "    patterns.append(torch.rot90(umbrella, rot))\n",
    "\n",
    "from PIL import Image   \n",
    "from IPython.display import display  \n",
    "from torchvision.transforms import ToPILImage\n",
    "as_image = ToPILImage()\n",
    "\n",
    "def show_image(img, magnify = 3):\n",
    "    as_image(img)\n",
    "    display(as_image(img).resize((img.shape[0]*magnify, img.shape[1]*magnify)))\n",
    "\n",
    "factor = 15\n",
    "img_side = factor*5\n",
    "img = torch.zeros(img_side, img_side) - 1\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(4444)\n",
    "\n",
    "mask_ys = np.random.choice(img_side//5, len(patterns), replace=False)\n",
    "mask_xs = np.random.choice(img_side//5, len(patterns), replace=False)\n",
    "\n",
    "for pattern, (y, x) in zip(patterns, zip(mask_ys, mask_xs)):\n",
    "    img[y*5 : y*5 + 5, x*5 : x*5 + 5] = pattern\n",
    "\n",
    "img = torch.nn.functional.pad(img, (5, 5, 5, 5), mode='constant', value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image `img` was formed by placing a bunch of $5 \\times 5$ emoji-style symbols as $+1$ patterns in a field of $âˆ’1$ values, and every\n",
    "point in both the input and output data is $\\pm1$. You can print the image to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85, 85])\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=255x255>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(img.shape)\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='red'>Exercise 4.1.1</font>:</b> \n",
    "Your task is to populate the weights and biases of a Conv2D filter to detect the `\"umbrella\"` pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(umbrella.to(torch.float32), magnify = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that all the emoji-style symbols are $5 \\times 5$ patterns. So, the filter should also have the same shape. In this simple case, \n",
    "your image has only one channel.\\\\\n",
    "Pytorch defines a 1-channel 5x5 convolution with padding of 2 as:\n",
    "\\begin{align}\n",
    "\\text{output}_{x,\\,y} = \\text{bias} + \\sum_{i=0}^{4} \\sum_{j=0}^{4} \\text{input}_{x+i-2,\\, y+j-2} \\times \\text{weight}_{i,\\,j}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "And, since you are looking for a single pattern, you will need only one filter. So, the number of output channels is also 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = torch.nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=1, \n",
    "    kernel_size=(5, 5),\n",
    "    padding=2, \n",
    "    bias=True\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# TODO: set the weights and bias of conv2d to the correct values\n",
    "# Maybe first you want to print the shapes of the weights and bias\n",
    "conv2d.weight.data = None\n",
    "conv2d.bias.data = None\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# call conv2d on img. The Conv2d layer expects a 4D tensor as input\n",
    "# batch_size x in_channels x img_height x img_width. `None`s are used to introduce extra dimensions.\n",
    "conv2d_output = torch.sign(conv2d(img[None, None, ...]).squeeze())\n",
    "\n",
    "# show the output\n",
    "# does it look like it detected the umbrellas (and only the umbrellas) at their right locattions?\n",
    "print(conv2d_output.shape)\n",
    "show_image(conv2d_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions to view network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "def view_network_parameters(model):\n",
    "    # Visualise the number of parameters\n",
    "    tensor_list = list(model.state_dict().items())\n",
    "    total_parameters = 0\n",
    "    print('Model Summary\\n')\n",
    "    for layer_tensor_name, tensor in tensor_list:\n",
    "        total_parameters += int(torch.numel(tensor))\n",
    "        print('{}: {} elements'.format(layer_tensor_name, torch.numel(tensor)))\n",
    "    print(f'\\nTotal Trainable Parameters: {total_parameters}!')\n",
    "\n",
    "def view_network_shapes(model, input_shape):\n",
    "    print(summary(model, input_size=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Network for Image Classification\n",
    "Assume that we have a 28x28 grascale image and we want to classify it into 10 classes\n",
    "\n",
    "So, the input shape is `(1, 28, 28)` and the output shape is `(10,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "input_shape = (1, 28, 28)\n",
    "num_classes = 10\n",
    "##########################################\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "def simple_fc_net():\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(1*28*28,8*28*28),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8*28*28,16*14*14),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*14*14,32*7*7),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32*7*7,288),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(288,64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,10),\n",
    "        nn.Softmax())\n",
    "    return model\n",
    "\n",
    "fc_net = simple_fc_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_network_parameters(fc_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_network_shapes(fc_net, torch.randn(input_shape).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='Red'>Exercise 4.1.2:</font> Now try to add different layers and see how the network parameters vary. Does adding layers reduce the parameters? Does the number of hidden neurons in the layers affect the total trainable parameters?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network for Image Classification\n",
    "\n",
    "Familiarize yourself with the different hyperparameters of `Conv2d` layer\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "Let's build a simple CNN to classify our images. \n",
    "\n",
    "<font size=4 color='red'>Exercise 4.1.3</font>:</b> \n",
    "Checkout the following code. The first layer is defined as \n",
    "\n",
    "```\n",
    "Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=8, \n",
    "    kernel_size=3,  \n",
    "    padding=1\n",
    ")\n",
    "```\n",
    "\n",
    "It takes in a 1x28x28 image and outputs a 8x28x28 image. Refer to the documentations to understand how the output shape is determined. \n",
    "\n",
    "Now, similar to `simple_fc_net`, add 3 more layers that has the following input and output shapes:\n",
    "```\n",
    "\n",
    "8x28x28 -> 16x14x14\n",
    "16x14x14 -> 32x7x7\n",
    "32x7x7 -> [Any num_channel x height x width factorization of 288]\n",
    "\n",
    "```\n",
    "\n",
    "Make sure that after flatten is called the output shape is 288.\n",
    "\n",
    "Dont forget to add `ReLU()` after each layer. \n",
    "\n",
    "<b>Extra-credit:</b> Try not to use MaxPool2d !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_conv_net():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1,8,kernel_size=3,padding=1),\n",
    "        nn.ReLU(),\n",
    "        # TODO: Add layers below such that when flatten() is called the output shapce is of size (batch_size, 288)\n",
    "        # ! Extra credit: Try not to use MaxPool2d \n",
    "        # Your code here\n",
    "        # ---------------------------------------------------------------\n",
    "        # Do not change the code below\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(288, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,10),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "conv_net = simple_conv_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_network_parameters(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_network_shapes(conv_net, input_shape=(1,1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='red'>Exercise 4.1.4</font>: What is the ratio of number of parameters of Conv-net to number of parameters of FC-Net? <br>\n",
    "\n",
    "---\n",
    "$\\frac{p_{conv-net}}{p_{fc-net}} = \\text{ANS}$\n",
    "\n",
    "---\n",
    "\n",
    "Do you see the difference ?! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Objectives\n",
    "\n",
    "-   Load dataset with PyTorch `dataloader` and `datasets`\n",
    "-   Define the CNN classifier, loss objective, and optimizer\n",
    "-   Train the model and validate the model\n",
    "-   Test on data that is different from the training data\n",
    "-   Improve model performance using data augmentation on training data\n",
    "\n",
    "Many of the exercises in this notebook are contained within code comments - so please read through the notebook carefully! If you have questions please ask on piazza or in a TA's office hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.2.1: Loading data\n",
    "\n",
    "We will use the MNIST dataset. The training consists of 60000 28x28 images in 10 classes. The dataset is roughly balanced.\n",
    "`torchvision` has a builtin function to load the dataset with `torchvision.datasets.MNIST` that takes care of necessary transformations as well. But, in this exercise, we will download the dataset as jpg images and load them using the `torchvision.datasets.ImageFolder` function. We will also define our own transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract the dataset\n",
    "# ! pip install gdown # colab should already have gdown installed. uncomment if you are running locally\n",
    "! pip install git+https://github.com/davidbau/baukit\n",
    "! pip install torchinfo\n",
    "! gdown https://drive.google.com/uc\\?id\\=1PX8mlpMk0cKL5fdQWXkimdcpg2Ht18aZ\n",
    "! unzip mnist_4440.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision, os, torchinfo\n",
    "from torchvision.datasets import ImageFolder\n",
    "from baukit import show\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"mnist_4440/training\"\n",
    "validation_path = \"mnist_4440/validation\"\n",
    "test_path = \"mnist_4440/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ImageFolder(train_path)\n",
    "print(\"Number of images in the training set =\", len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the PIL image and the class name.\n",
    "idx = 14440\n",
    "item = train_set[idx]\n",
    "print(f\"{idx}th item is a pair\", item)\n",
    "\n",
    "img, label = train_set[idx]\n",
    "show(img)\n",
    "print(f\"{label=} ({train_set.classes[label]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a model, we need to convert the images to PyTorch tensors.\n",
    "from torchvision import transforms\n",
    "\n",
    "transforms.functional.pil_to_tensor(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the `torchvision.transforms.ToTensor` transform\n",
    "# to convert the PIL image to a PyTorch tensor directly while loading\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_set = torchvision.datasets.ImageFolder(train_path, transform=ToTensor())\n",
    "img, label = train_set[idx]\n",
    "\n",
    "# now the image is a tensor\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an inverse transform that can be used to convert it back to a PIL image,\n",
    "# handy if we want to see it.\n",
    "\n",
    "as_image = transforms.ToPILImage()\n",
    "show(as_image(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `DataLoader` to load the images faster in batches.\n",
    "\n",
    "Checkout this links to understand how `DataLoader` and `Datasets` work in Pytorch.\n",
    "\n",
    "-   [Pytorch Tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "-   [David's Tips on this](https://github.com/davidbau/how-to-read-pytorch/blob/master/notebooks/5-Pytorch-Dataloader.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider reducing the batch size if you run out of memory\n",
    "# or increasing if you have more memory and want to speed up\n",
    "\n",
    "train_set = torchvision.datasets.ImageFolder(\n",
    "    train_path,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "##########################################################\n",
    "# **Exercise 4.2.1.1**\n",
    "# TODO: load the validation set in the `val_set` variable\n",
    "# and create a dataloader for it in the `val_loader` variable\n",
    "# use the `validation_path` from above\n",
    "# ! Please don't do any fancy transforms/augmentations, just convert the images to tensors\n",
    "\n",
    "val_set = None\n",
    "val_loader = None\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first batch returned by the loader\n",
    "images, labels = next(train_loader.__iter__())\n",
    "print(f\"{images.shape=}, {labels.shape=}\")\n",
    "\n",
    "# the shape of images is batch_size x channels x height x width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data distribution in the training set\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "counter = torch.zeros(len(train_set.classes))\n",
    "for images, labels in tqdm(train_loader):\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "##########################################################\n",
    "# **Exercise 4.2.1.2**\n",
    "# TODO: Plot the distribution of classes in the training set as a bar plot\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html\n",
    "\n",
    "raise NotImplementedError\n",
    "\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.2.2: Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Some helper functions to view network parameters\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "def view_network_shapes(model, input_shape):\n",
    "    print(summary(model, input_size=input_shape))\n",
    "\n",
    "\n",
    "def view_network_parameters(model):\n",
    "    # Visualise the number of parameters\n",
    "    tensor_list = list(model.state_dict().items())\n",
    "    total_parameters = 0\n",
    "    print(\"Model Summary\\n\")\n",
    "    for layer_tensor_name, tensor in tensor_list:\n",
    "        total_parameters += int(torch.numel(tensor))\n",
    "        print(\"{}: {} elements\".format(layer_tensor_name, torch.numel(tensor)))\n",
    "    print(f\"\\nTotal Trainable Parameters: {total_parameters}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_classifier():\n",
    "    torch.manual_seed(4440)\n",
    "    layers = [\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(3 * 28 * 28, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 10),\n",
    "    ]\n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "mlp = mlp_classifier().to(device)\n",
    "view_network_shapes(mlp, img[None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def cnn_classifier():\n",
    "    torch.manual_seed(4440)  # for reproducibility\n",
    "    ##########################################################################\n",
    "    # **Exercise 4.2.2.1**\n",
    "    # TODO: Make a CNN classifier for the task wrapped in an nn.Sequential module. \n",
    "    # You can vary your architecture choices (# layers, # convs, activation functions, regularization, etc.)\n",
    "    # However, your parameter size should not be more than 0.5 MB. Use the `view_network_shapes` function to get the `Params size`\n",
    "    model = None\n",
    "    ##########################################################################\n",
    "    return model\n",
    "\n",
    "\n",
    "model = cnn_classifier().to(device)\n",
    "\n",
    "view_network_shapes(model, img[None].shape)\n",
    "\n",
    "# ! DEBUG TIPS: In case you get an error with `view_network_shapes`,\n",
    "# Comment out different layers in the model definition, call the network on random input of the right shape\n",
    "# and check how far it goes.\n",
    "\n",
    "# model(torch.randn_like(img)[None].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    # Initiate a loss monitor\n",
    "    train_loss = []\n",
    "    correct_predictions = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # predict the class\n",
    "        predicted = model(images)\n",
    "        loss = loss_fn(predicted, labels)\n",
    "        correct_predictions += (predicted.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        # Backward pass (back propagation)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(train_loss), correct_predictions / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "# evaluating the model\n",
    "@torch.inference_mode()  # understand what torch.inference_mode() or torch.no_grad() does\n",
    "def evaluate_model(model, val_loader, loss_fn, return_confusion_matrix=False):\n",
    "    model.eval()\n",
    "    # Initiate a loss monitor\n",
    "    val_loss = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    if return_confusion_matrix:\n",
    "        confusion_matrix = torch.zeros(\n",
    "            len(val_loader.dataset.classes), len(val_loader.dataset.classes)\n",
    "        )\n",
    "\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # predict the class\n",
    "        predicted = model(images)\n",
    "        loss = loss_fn(predicted, labels)\n",
    "        correct_predictions += (predicted.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        if return_confusion_matrix:\n",
    "            for t, p in zip(labels.view(-1), predicted.argmax(dim=1).view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_acc = correct_predictions / len(val_loader.dataset)\n",
    "\n",
    "    if return_confusion_matrix:\n",
    "        return val_loss, val_acc, confusion_matrix\n",
    "    else:\n",
    "        return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4440)\n",
    "\n",
    "model = cnn_classifier().to(device)\n",
    "epochs = 10  # your model should be able to get > 98% validation accuracy on the first 10 epochs\n",
    "####################################################################\n",
    "# **Exercise 4.2.2.2**\n",
    "# TODO: choose values for your hyperparameters\n",
    "learning_rate = None\n",
    "weight_decay = None\n",
    "# TODO: choose a loss function and optimizer\n",
    "loss_fn = None\n",
    "optimizer = None\n",
    "####################################################################\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_model(model, train_loader, loss_fn, optimizer)\n",
    "    training_losses.append(train_loss)\n",
    "    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn)\n",
    "    validation_losses.append(val_loss)\n",
    "    print(\n",
    "        f\"epoch: {epoch+1}/{epochs} | train loss={np.mean(train_loss):.4f}, {train_acc=:.4f} | val loss: {np.mean(val_loss):.4f}, {val_acc=:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# **Exercise 4.2.2.3**\n",
    "# TODO: Plot the training and validation losses vs. training epochs on the same plot\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "\n",
    "raise NotImplementedError\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(confusion_matrix, labels, cmap=\"Blues\"):\n",
    "    confusion_matrix = np.array(confusion_matrix)\n",
    "    assert confusion_matrix.shape[0] == confusion_matrix.shape[1]\n",
    "    assert confusion_matrix.shape[0] == len(labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    plt.imshow(confusion_matrix, cmap=cmap)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(val_set.classes)), labels)\n",
    "    ax.set_yticks(np.arange(len(val_set.classes)), labels)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "\n",
    "    for true_label in range(len(val_set.classes)):\n",
    "        for pred_label in range(len(val_set.classes)):\n",
    "            ax.text(\n",
    "                pred_label,\n",
    "                true_label,\n",
    "                int(confusion_matrix[true_label, pred_label]),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"black\",\n",
    "            )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc, confusion_matrix = evaluate_model(\n",
    "    model, val_loader, loss_fn, return_confusion_matrix=True\n",
    ")\n",
    "print(f\"{val_loss=}, {val_acc=}\")\n",
    "draw_confusion_matrix(confusion_matrix, train_set.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4440)\n",
    "\n",
    "########################################################\n",
    "# **Exercise 4.2.2.4**\n",
    "# TODO: Load the test set in the `test_set` variable\n",
    "test_set = None\n",
    "test_loader = None\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# **Exercise 4.2.2.5**\n",
    "# TODO: Evaluate the model on the test set\n",
    "# print the test loss and accuracy. \n",
    "# Also, draw the confusion matrix\n",
    "# Check how it was done for the validation set\n",
    "\n",
    "raise NotImplementedError\n",
    "\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are the results on the test set different from the validation set?\n",
    "# visualize the training images\n",
    "\n",
    "torch.manual_seed(4440)\n",
    "\n",
    "training_images, training_labels = next(iter(train_loader))\n",
    "\n",
    "show(\n",
    "    show.TIGHT,\n",
    "    [\n",
    "        [\n",
    "            [f\"{train_set.classes[label]}\", as_image(img)]\n",
    "            for img, label in list(zip(training_images, training_labels))[:20]\n",
    "        ]\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4440)\n",
    "\n",
    "###########################################################\n",
    "# **Exercise 4.2.2.6**\n",
    "# TODO: Visualize the test images\n",
    "# Are they slightly different from the training images?\n",
    "\n",
    "raise NotImplementedError\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.2.3: Data Augmentation to improve model performance\n",
    "\n",
    "Often times, when you deploy your model in the real world, the data it sees is different from the data it was trained on. Data Augmentation a collection of techniques to perform random transformations on the training data to make the model more robust to slight changes. It reduces overfitting and improves generalization.\n",
    "\n",
    "It can also be handy if you have a small dataset and you want to increase the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    RandomRotation,\n",
    ")\n",
    "\n",
    "torch.manual_seed(4440)\n",
    "\n",
    "# Play with different augmentations\n",
    "# Checkout https://pytorch.org/vision/stable/transforms.html for transformation options\n",
    "\n",
    "augmentations = {\n",
    "    \"translate\": RandomCrop(size=28, padding=5),\n",
    "    \"hflip\": RandomHorizontalFlip(p=1),\n",
    "    \"vflip\": RandomVerticalFlip(p=1),\n",
    "    \"rotate\": RandomRotation(20),\n",
    "}\n",
    "\n",
    "augmented_images = {k: func(img) for k, func in augmentations.items()}\n",
    "\n",
    "show(\n",
    "    show.TIGHT,\n",
    "    [\n",
    "        [[\"original\", as_image(img)]]\n",
    "        + [\n",
    "            [\n",
    "                f\"{aug} => {aimg.shape[0]}x{aimg.shape[1]}x{aimg.shape[2]}\",\n",
    "                as_image(aimg),\n",
    "            ]\n",
    "            for aug, aimg in augmented_images.items()\n",
    "        ]\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4440)\n",
    "# You can also combine multiple augmentations\n",
    "\n",
    "aug = transforms.Compose(\n",
    "    [\n",
    "        RandomRotation(100),\n",
    "        RandomCrop(size=28, padding=5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "show(\n",
    "    show.TIGHT,\n",
    "    [[[\"original\", as_image(img)], [\"rotate > translate\", as_image(aug(img))]]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# **Exercise 4.2.3.1**\n",
    "# TODO: Load the training and validation data with appropriate augmentations\n",
    "# Visualize more test images to understand how they are different from the training images\n",
    "# And, then train the images with the augmented data\n",
    "\n",
    "train_set = None\n",
    "train_loader = None\n",
    "\n",
    "val_set = None\n",
    "val_loader = None\n",
    "\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4440)\n",
    "# training augmented the model.\n",
    "aug_model = cnn_classifier().to(device)\n",
    "\n",
    "epochs = 10\n",
    "####################################################################\n",
    "# TODO: Use the same hparams as before\n",
    "# You can replace the Nones with the right values\n",
    "# Or, just comment them out and they will used the values you defined before\n",
    "learning_rate = None\n",
    "weight_decay = None\n",
    "loss_fn = None\n",
    "\n",
    "# Don't forget to pass aug_model.parameters() now, instead of model.parameters()\n",
    "optimizer = None\n",
    "####################################################################\n",
    "\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_model(aug_model, train_loader, loss_fn, optimizer)\n",
    "    training_losses.append(train_loss)\n",
    "    val_loss, val_acc = evaluate_model(aug_model, val_loader, loss_fn)\n",
    "    validation_losses.append(val_loss)\n",
    "    print(\n",
    "        f\"epoch: {epoch+1}/{epochs} | train loss={np.mean(train_loss):.4f}, {train_acc=:.4f} | val loss: {np.mean(val_loss):.4f}, {val_acc=:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# **Exercise 4.2.3.2**\n",
    "# TODO: Evaluate your `aug_model` on the test set\n",
    "# print the test loss and accuracy.\n",
    "# Also, draw the confusion matrix\n",
    "# Expect > 95% test accuracy\n",
    "\n",
    "raise NotImplementedError\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2.3.3**\n",
    "\n",
    "Analyze the confusion matrix you made in part 4.2.2.5:\n",
    "- What sorts of errors was your classifier making when trained on the original data? \n",
    "- Are there particular classes that your model was confusing? \n",
    "- When comparing to the confusion matrix in exercise 4.2.3.2, does it look like your data augmentations helped? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2.3.3 Response:**\n",
    "\n",
    "- Enter your Response Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, PIL.Image, numpy\n",
    "from torchvision.models import alexnet, resnet18, resnet101, resnet152, efficientnet_b1\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, CenterCrop\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to download necessary files\n",
    "# ! wget -N https://cs7150.baulab.info/2022-Fall/data/dog-and-cat-example.jpg\n",
    "# ! wget -N https://cs7150.baulab.info/2022-Fall/data/hungry-cat.jpg\n",
    "# ! wget -N https://cs7150.baulab.info/2022-Fall/data/imagenet-labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the behavior of a convolutional network\n",
    "\n",
    "In this exercise, we will try to understand how a convolutional network behaves after it has finished training. We will brieflly overview some of the major categories of methods for visualizing the behavior of a CNN: occlusion, gradients, class activation maps (CAM), and dissection.\n",
    "\n",
    "Let's define some utility functions for manipulating images. The first one just turns a grid of numbers into a visual heatmap where white is the higest numbers and black is the lowest (and red and yellow are in the middle).\n",
    "\n",
    "Another is for making a theshold mask instead of a heatmap, to just highlight the highest regions.\n",
    "\n",
    "And then another one creates an overlay between two images.\n",
    "\n",
    "With these in hand, we can create some salience map visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_heatmap(\n",
    "    data,\n",
    "    size=None,\n",
    "    colormap=\"hot\",\n",
    "    amax=None,\n",
    "    amin=None,\n",
    "    mode=\"bicubic\",\n",
    "    symmetric=False,\n",
    "):\n",
    "    size = spec_size(size)\n",
    "    mapping = getattr(cm, colormap)\n",
    "    scaled = torch.nn.functional.interpolate(data[None, None], size=size, mode=mode)[\n",
    "        0, 0\n",
    "    ]\n",
    "    if amax is None:\n",
    "        amax = data.max()\n",
    "    if amin is None:\n",
    "        amin = data.min()\n",
    "    if symmetric:\n",
    "        amax = max(amax, -amin)\n",
    "        amin = min(amin, -amax)\n",
    "    normed = (scaled - amin) / (amax - amin + 1e-10)\n",
    "    return PIL.Image.fromarray((255 * mapping(normed)).astype(\"uint8\"))\n",
    "\n",
    "\n",
    "def rgb_threshold(data, size=None, mode=\"bicubic\", p=0.2):\n",
    "    size = spec_size(size)\n",
    "    scaled = torch.nn.functional.interpolate(data[None, None], size=size, mode=mode)[\n",
    "        0, 0\n",
    "    ]\n",
    "    ordered = scaled.view(-1).sort()[0]\n",
    "    threshold = ordered[int(len(ordered) * (1 - p))]\n",
    "    result = numpy.tile((scaled > threshold)[:, :, None], (1, 1, 3))\n",
    "    return PIL.Image.fromarray((255 * result).astype(\"uint8\"))\n",
    "\n",
    "\n",
    "def overlay(im1, im2, alpha=0.5):\n",
    "    import numpy\n",
    "\n",
    "    return PIL.Image.fromarray(\n",
    "        (\n",
    "            numpy.array(im1)[..., :3] * alpha + numpy.array(im2)[..., :3] * (1 - alpha)\n",
    "        ).astype(\"uint8\")\n",
    "    )\n",
    "\n",
    "\n",
    "def overlay_threshold(im1, im2, alpha=0.5):\n",
    "    import numpy\n",
    "\n",
    "    return PIL.Image.fromarray(\n",
    "        (\n",
    "            numpy.array(im1)[..., :3] * (1 - numpy.array(im2)[..., :3] / 255) * alpha\n",
    "            + numpy.array(im2)[..., :3] * (numpy.array(im1)[..., :3] / 255)\n",
    "        ).astype(\"uint8\")\n",
    "    )\n",
    "\n",
    "\n",
    "def spec_size(size):\n",
    "    if isinstance(size, int):\n",
    "        dims = (size, size)\n",
    "    if isinstance(size, torch.Tensor):\n",
    "        size = size.shape[:2]\n",
    "    if isinstance(size, PIL.Image.Image):\n",
    "        size = (size.size[1], size.size[0])\n",
    "    if size is None:\n",
    "        size = (224, 224)\n",
    "    return size\n",
    "\n",
    "\n",
    "def resize_and_crop(im, d):\n",
    "    if im.size[0] >= im.size[1]:\n",
    "        im = im.resize((int(im.size[0] / im.size[1] * d), d))\n",
    "        return im.crop(((im.size[0] - d) // 2, 0, (im.size[0] + d) // 2, d))\n",
    "    else:\n",
    "        im = im.resize((d, int(im.size[1] / im.size[9] * d)))\n",
    "        return im.crop((0, (im.size[1] - d) // 2, d, (im.size[1] + d) // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pretrained classifier and an example image\n",
    "\n",
    "Here is an example image, and an example network.\n",
    "\n",
    "We will look at a `resnet18`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = resize_and_crop(PIL.Image.open(\"dog-and-cat-example.jpg\"), 224)\n",
    "show(im)\n",
    "data = renormalize.from_image(resize_and_crop(im, 224), target=\"imagenet\")\n",
    "with open(\"imagenet-labels.txt\") as r:\n",
    "    labels = [line.split(\",\")[1].strip() for line in r.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "net = resnet18(pretrained=True)\n",
    "net.eval()  # inference mode. turn off dropout, batchnorm, etc.\n",
    "set_requires_grad(\n",
    "    False, net\n",
    ")  # you will not be training the net, turn off gradients to save some memory\n",
    "\n",
    "# net # print the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using occlusion\n",
    "\n",
    "First, let's try a method suggested by [Zeiler 2014](https://arxiv.org/pdf/1311.2901.pdf).  Slide a window across the image and test each version.\n",
    "\n",
    "The following is a function for creating a series of sliding-window masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(dims=None, window=1, stride=1, hole=True):\n",
    "    dims = spec_size(dims)\n",
    "    assert len(dims) == 2\n",
    "    for y in range(0, dims[0], stride):\n",
    "        for x in range(0, dims[1], stride):\n",
    "            mask = torch.zeros(*dims)\n",
    "            mask[y : y + window, x : x + window] = 1\n",
    "            if hole:\n",
    "                mask = 1 - mask\n",
    "            yield mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a batch of masks,   and then we will create a `masked_batch` batch of images which have a gray square masked in in each of them.  We will create some 196 versions of this masked image.\n",
    "\n",
    "Below is an example picture of one of the masked images, where the mask happens to cover the dog's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = torch.stack(list(sliding_window(im, window=48, stride=16)))\n",
    "masks = masks[:, None, :, :]\n",
    "print(\"masks\", masks.shape)\n",
    "\n",
    "masked_batch = data * masks\n",
    "print(\"masked_batch\", masked_batch.shape)\n",
    "\n",
    "show(\n",
    "    renormalize.as_image(masked_batch[75])\n",
    ")  # play with the index to see different masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the network to get its predictions.\n",
    "\n",
    "But also we will run the network on each of the masked images.\n",
    "\n",
    "Notice that this image is guessed as both a dog ('boxer') and cat ('tiger cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_preds = net(data[None])  # base prediction of the original image\n",
    "[(labels[i], i.item()) for i in base_preds.topk(dim=1, k=5, sorted=True)[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_preds = net(masked_batch)  # prediction of the masked images\n",
    "len(masked_preds)  # 196 predictions, one for each mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask 19 hides the face of the dog.\n",
    "dog_face_mask_idx = 19\n",
    "show(renormalize.as_image(masked_batch[dog_face_mask_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='Red'>Exercise 4.3.1:</font> What are the predictions of the network for the masked image shown above? Print them out like we did above. What do you think happened here? Give your thoughts\n",
    "\n",
    "> **Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get the predictions when the dog's face is masked\n",
    "\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='Red'>Exercise 4.3.2:</font> For each of the masked image, we have predictions. \n",
    "- Show the image that has least score for the label `boxer`\n",
    "- Show the image that has most score for the label `tiger cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show the masked image with LEAST score for `boxer`\n",
    "\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show the masked image with MOST score for `tiger cat`\n",
    "\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way that we can visualise the pixels that are more responsible for the predictions. It's something similar you did above in Exercise 4.3.1 and 4.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"boxer\", \"tiger cat\"]:\n",
    "    heatmap = (base_preds[:, labels.index(c)] - masked_preds[:, labels.index(c)]).view(\n",
    "        14, 14\n",
    "    )\n",
    "    show(\n",
    "        show.TIGHT,\n",
    "        [\n",
    "            [\n",
    "                [c, rgb_heatmap(heatmap, mode=\"nearest\", symmetric=True)],\n",
    "                [\"ovarlay\", overlay(im, rgb_heatmap(heatmap, symmetric=True))],\n",
    "            ]\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using smoothgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neural networks are differentiable, it is natural to try to visualize them using gradients.\n",
    "\n",
    "One simple method is smoothgrad ([Smilkov 2017](https://arxiv.org/pdf/1706.03825.pdf)), which examines gradients of perturbed inputs.\n",
    "\n",
    "The concept is, \"according to gradients, which pixels most affect the prediction of the given class?\"\n",
    "\n",
    "Although gradients are a neat idea, it can be hard to get them to work well for visualization.  See [Adebayo 2018](https://arxiv.org/pdf/1810.03292.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color=\"red\">Exercise 4.3.3</font>: In this exercise, we will see the gradient wrt to the image. Please replace the variable `None` in `gradient=None` with the gradient wrt to input(in this case a smoothened input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"boxer\", \"tiger cat\"]:\n",
    "    total = 0\n",
    "    for i in range(20):\n",
    "        prober = data + torch.randn(data.shape) * 0.2\n",
    "        prober.requires_grad = True\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            net(prober[None]), torch.tensor([labels.index(label)])\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO Replace None with the gradient wrt to the perturbed input\n",
    "        gradient = None\n",
    "        ######################################################################\n",
    "\n",
    "        total += gradient**2\n",
    "        prober.grad = None\n",
    "\n",
    "    show(\n",
    "        show.TIGHT,\n",
    "        [\n",
    "            [\n",
    "                [label, renormalize.as_image(data, source=\"imagenet\")],\n",
    "                [\n",
    "                    \"total grad**2\",\n",
    "                    renormalize.as_image(\n",
    "                        (total / total.max() * 5).clamp(0, 1), source=\"pt\"\n",
    "                    ),\n",
    "                ],\n",
    "                [\n",
    "                    \"overlay\",\n",
    "                    overlay(\n",
    "                        renormalize.as_image(data, source=\"imagenet\"),\n",
    "                        renormalize.as_image(\n",
    "                            (total / total.max() * 5).clamp(0, 1), source=\"pt\"\n",
    "                        ),\n",
    "                    ),\n",
    "                ],\n",
    "            ]\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single neuron dissection\n",
    "\n",
    "In this code, we ask \"What does a single kind of neuron detect?\", e.g., the neurons of the 100th convolutional filter of the layer4.0.conv1 layer of resnet18.\n",
    "\n",
    "To see that, we use dissection to visualize the neurons ([Bau 2017](https://arxiv.org/pdf/1704.05796.pdf)).\n",
    "\n",
    "We run the network over a large sample of images (here we use 5000 random images from the imagenet validation set), and we show the 12 regions where the neuron activated strongest in this data set.\n",
    "\n",
    "Can you see a pattern for neuron 100?  What about for neuron 200 or neuron 50?\n",
    "\n",
    "Some neurons activate on more than one concept.  Some neurons are more understandable than others.\n",
    "\n",
    "Below, we begin by loading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"imagenet_val_5k\"):\n",
    "    download_and_extract_archive(\n",
    "        \"https://cs7150.baulab.info/2022-Fall/data/imagenet_val_5k.zip\",\n",
    "        \"imagenet_val_5k\",\n",
    "    )\n",
    "ds = ImageFolderSet(\n",
    "    \"imagenet_val_5k\",\n",
    "    shuffle=True,\n",
    "    transform=Compose(\n",
    "        [Resize(256), CenterCrop(224), ToTensor(), renormalize.NORMALIZER[\"imagenet\"]]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code examines the top-activating neurons in a particular convolutional layer, for our test image.\n",
    "\n",
    "Which is the first neuron that activates for the cat but not the dog?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dissect the first filter output of `layer4.1.conv1` and see what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"layer4.1.conv1\"\n",
    "filter_num = 0\n",
    "with Trace(net, layer) as tr:\n",
    "    preds = net(data[None])\n",
    "show(\n",
    "    show.WRAP,\n",
    "    [[f\"neuron {filter_num}\", overlay(im, rgb_heatmap(tr.output[0, filter_num]))]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='red'>Exercise 4.3.4</font>: The above representation is for filter 0. Now visualise the top 12 filters that activate the most. <br>\n",
    "[**Hint:** To do this, use max activations of each filter and show the top 12 filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find 12 filters of layer4.1.conv1 that are most activated by the image.\n",
    "\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='red'>Exercise 4.3.5</font>: Which of the top filters is activating the cat more? \n",
    "\n",
    "> **Answer:**\n",
    "\n",
    "Choose one or two and run the network on all the data and sort to find the maximum-activating data. Let's see how the neuron you found to be top activating generalizes. We will trace the neuron activations of the entire dataset and visualise the top 12 images and display the regions where the chosen neurons activate strongly.\n",
    "\n",
    "Here we select filter number 0 in `layer4.1.conv1` to show how you can do it. Replace it with the number you found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissect_unit(ds, i, net, layer, unit):\n",
    "    data = ds[i][0]\n",
    "    with Trace(net, layer) as tr:\n",
    "        net(data[None])\n",
    "    mask = rgb_threshold(tr.output[0, unit], size=data.shape[-2:])\n",
    "    img = renormalize.as_image(data, source=ds)\n",
    "    return overlay_threshold(img, mask)\n",
    "\n",
    "\n",
    "# TODO: Replace with the filter you found\n",
    "filter_no = 0\n",
    "##########################################\n",
    "\n",
    "scores = []\n",
    "for imagenum, [\n",
    "    d,\n",
    "] in enumerate(pbar(ds)):\n",
    "    with Trace(net, layer) as tr:\n",
    "        _ = net(d[None])\n",
    "    score = tr.output[0, filter_no].view(-1).max()\n",
    "    scores.append((score, imagenum))\n",
    "scores.sort(reverse=True)\n",
    "\n",
    "show(\n",
    "    f\"{layer} neuron {filter_no}\",\n",
    "    [[dissect_unit(ds, scores[i][1], net, layer, filter_no) for i in range(12)]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>Exercise 4.3.6</font>: Is the neuron only activating cats? How well do you think it is generalising?\n",
    "\n",
    "> **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using grad-cam\n",
    "\n",
    "Another idea is to look at gradients to the interior activations rather than gradients all the way to the pixels.  CAM ([Zhou 2015](https://arxiv.org/pdf/1512.04150.pdf)) and Grad-CAM ([Selvaraju 2016](https://arxiv.org/pdf/1610.02391.pdf)) do that.\n",
    "\n",
    "Grad-cam works by examiming internal network activations; to do that we will use the `Trace` class from `baukit`.\n",
    "\n",
    "So we run the network again in inference to classify the image, this time tracing the output of the last convolutional layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Trace(net, \"layer4\") as tr:\n",
    "    preds = net(data[None])\n",
    "print(\"The output of layer4 is a set of neuron activations of shape\", tr.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we make sense of these 512-dimenaional vectors?  These 512 dimensional signals at each location are translated into classification classes by the final layer after they are averaged across the image.  Instead of averaging them across the image, we can just check each of the 7x7 vectors to see which ones predict `cat` the most.  Or we can do the same thing for `dog` (`boxer`).\n",
    "\n",
    "The first step is to get the neuron weights for the cat and the dog neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxer_weights = net.fc.weight[labels.index(\"boxer\")]\n",
    "boxer_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the weight vectors has 512 dimensions, reflecting all the input weights for each of the neurons.\n",
    "\n",
    "The second step is to dot product (matrix-multply) these weights to each of the 7x7 vectors, each of which is also 512 dimensions.\n",
    "\n",
    "The result will be a 7x7 grid of dot product strengths, which we can render as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxer_heatmap = torch.einsum(\"bcyx, c -> yx\", tr.output, boxer_weights)\n",
    "print(f\"{boxer_heatmap.shape=}\")\n",
    "\n",
    "show(show.TIGHT, [[\"boxer\", rgb_heatmap(boxer_heatmap, mode=\"nearest\")]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we smooth the heatmaps and overlay them on top of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\n",
    "    show.TIGHT,\n",
    "    [[\n",
    "        [\"original\", im], \n",
    "        [\"boxer\", overlay(im, rgb_heatmap(boxer_heatmap, im))]\n",
    "    ]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='red'>Exercise 4.3.7</font>: Repeat the grad-cam on the `tiger-cal` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: grad-cam on the `tiger-cat` class\n",
    "\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='red'>Exercise 4.3.8</font>: Now consider the image hungry-cat.jpg\n",
    "\n",
    "Load the image `hungry-cat.jpg` and use grad-cam to visualize the heatmap for the tiger cat and goldfish classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  load the `hungry-cat.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show the grad-cam on the `tiger-cat` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO show the grad-cam on the `goldfish` class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
